#### PradOp

- Manages the computational graph and operations.
- Handles forward and backward passes.
- Supports branching and splitting for complex graph structures.

#### PradResult

- Encapsulates the result of a computation.
- Provides methods for backpropagation and chaining operations.

### Constructor 

```csharp 
public PradOp(Tensor seed) 
``` 

Creates a new instance of the `PradOp` class with a seed tensor. 

- `seed`: The initial tensor to start computations with. 

### Properties 

| Property | Type | Description | 
|----------|------|-------------| 
| `UpstreamGradient` | `Tensor` | Gets or sets the upstream gradient for backpropagation. | 
| `SeedGradient` | `Tensor` | Gets or sets the gradient of the seed tensor. | 
| `IsDependentBranch` | `bool` | Indicates whether this is a dependent branch in the computation graph. | 
| `CurrentShape` | `int[]` | Gets the shape of the current tensor. | 
| `Id` | `Guid` | Gets the unique identifier of the PradOp instance. |  
| `Result` | `Tensor?` | Gets the result of the computation. | 
| `CurrentTensor` | `Tensor` | Gets the tensor result of the computation or the seed tensor if no computation has been performed. | 

### Methods 

#### Tensor Operations 

| Method | Description | 
|--------|-------------| 
| `Add(Tensor tensor)` | Adds the given tensor to the current tensor element-wise. | 
| `Sub(Tensor tensor)` | Subtracts the given tensor from the current tensor element-wise. | 
| `SubFrom(Tensor tensor)` | Subtracts the current tensor from the given tensor element-wise. | 
| `Mul(Tensor tensor)` | Multiplies the current tensor by the given tensor element-wise. | 
| `Div(Tensor tensor)` | Divides the current tensor by the given tensor element-wise. | 
| `DivInto(Tensor tensor)` | Divides the given tensor by the current tensor element-wise. | 
| `MatMul(Tensor tensor)` | Performs matrix multiplication of the current tensor with the given tensor. | 
| `Sin()` | Computes the sine of each element in the current tensor. | 
| `Cos()` | Computes the cosine of each element in the current tensor. | 
| `Atan2(Tensor tensor)` | Computes the arctangent of the quotient of the current tensor and the given tensor. | 
| `Square()` | Computes the square of each element in the current tensor. | 
| `SquareRoot()` | Computes the square root of each element in the current tensor. | 
| `SumRows()` | Sums the rows of the current tensor. | 
| `Exp()` | Computes the exponential of each element in the current tensor. | 
| `Ln()` | Computes the natural logarithm of each element in the current tensor. | 
| `Log()` | Computes the base-10 logarithm of each element in the current tensor. | 
| `TanH()` | Computes the tanH of each element in the current tensor. | 
| `LeakyReLU()` | Computes the leaky ReLU of each element in the current tensor. | 
| `Sigmoid()` | Computes the sigmoid of each element in the current tensor. | 
| `Mean(int axis)` | Computes the mean along the specified axis in the current tensor. | 
| `Reciprocal()` | Computes the reciprocal of each element in the current tensor. | 
| `Clip(double min, double max)` | Clips values to the specified range. | 
| `Exclude(double min, double max)` | Excludes values within the specified range. | 
| `Sum(int[] axes)` | Sums the tensor along specified axes. | 
| `BroadcastTo(int[] newShape)` | Broadcasts the tensor to a new shape. |
| `LessThan(Tensor tensor)` | Performs element-wise "less than" comparison. |
| `Where(Tensor condition, Tensor other)` | Selects elements based on a condition tensor. |
| `Modulus(Tensor tensor)` | Performs element-wise modulus operation. |
| `ExtractPatches(int[] filterSize, int[] strides, string padding)` | Extracts patches from a tensor for im2col. |
| `Max(Tensor other)` | Performs element-wise max operation. |
| `Min(Tensor other)` | Performs element-wise min operation. |
| `Abs()` | Performs element-wise absolute value operation. |
| `Pow(Tensor tensor)` | Performs an element-wise power operation on the current tensor with the provided exponent. |

#### Tensor Manipulation 

| Method | Description | 
|--------|-------------| 
| `Indexer(params string[] indices)` | Slices the tensor using the given indices. | 
| `Reshape(int[] newShape)` | Reshapes the current tensor to the specified shape. | 
| `Transpose(params int[] permutations)` | Transposes the current tensor according to the given permutations. | 
| `Split(int groupSize, int axis = 0)` | Splits the tensor into multiple tensors along the specified axis. | 
| `Tile(int[] multiples)` | Tiles the tensor along each dimension according to the given multiples. | 
| `PairwiseTile(Tensor other)` | Generates a 2 row tensor that represents all possible pairings between two 1D tensors. | 
| `Gather(Tensor indices, int axis = 0)` | Gathers slices from the tensor along the specified axis. | 
| `GatherNd(Tensor indices)` | Gathers slices from the tensor using multidimensional indices. | 
| `Slice(int[] begin, int[] size, int[]? strides = null)` | Extracts a slice from the tensor. | 
| `Stack(Tensor[] tensors, int axis = 0)` | Stacks the current tensor with other tensors along a new axis. | 
| `Concat(Tensor[] tensors, int axis = 0)` | Concatenates the current tensor with other tensors along a specified axis. | 
| `ExpandDims(int axis = -1)` | Expands the dimensions of the tensor along the specified axis. | 

#### Computation Graph Management 

| Method | Description | 
|--------|-------------| 
| `Branch()` | Creates a new branch in the computation graph. | 
| `DoParallel(params Func<PradOp, PradResult>[] operations)` | Executes multiple operations in parallel. | 
| `Back(Tensor tensor)` | Initiates backpropagation with the given upstream gradient. | 
| `Back()` | Computes backpropagation to accumulate gradients. | 
| `BranchStack(int n)` | Creates a specified number (n) of branches from the current PradOp instance and returns a BranchStack object for managing these branches. This allows you to easily pop branches as needed. |

### Static Operations 

`PradOp` provides several static operation delegates that can be used with the `Then` method of `PradResult`: 

- `SquareRootOp` 
- `AddOp` 
- `MulOp` 
- `SubOp`
- `SubFromOp`
- `DivOp`
- `DivIntoOp`
- `MatMulOp`
- `ExpandDimsOp`
- `SinOp` 
- `CosOp`
- `ReciprocalOp`
- `ExpOp`
- `LnOp`
- `LogOp`
- `MeanOp`
- `GatherOp`
- `GatherNdOp` 
- `SumRowsOp` 
- `SquareOp` 
- `Atan2Op` 
- `StackOp` 
- `ConcatOp`
- `IndexerOp`
- `ReshapeOp`
- `TransposeOp`
- `TileOp`
- `PairwiseTileOp`
- `ClipOp`
- `ExcludeOp`
- `SumOp`
- `BroadcastToOp`
- `LessThanOp`
- `WhereOp`
- `ModulusOp`
- `ExtractPatchesOp`
- `MaxOp`
- `MinOp`
- `AbsOp`
- `PowOp`
- `TanHOp`
- `LeakyReLUOp`
- `SigmoidOp`

### PradResult.Then Method

The `Then` method in `PradResult` is a powerful feature that allows for elegant chaining of operations in the computational graph. It provides a fluent interface for applying successive operations to the result of previous computations.

#### Method Signatures

```csharp
public PradResult Then(Delegate operation, Tensor? other = null)
public PradResult Then(Delegate operation, Tensor[] others, int axis = 0)
public PradResult Then(Func<PradResult[], PradResult> operation)
public PradResult[] Then(Func<PradResult[], PradResult[]> operation)
```

#### Functionality

1. **Chaining Operations**: The `Then` method allows you to apply a new operation to the result of the previous operation. This creates a chain of operations that can be read from left to right, improving code readability.

2. **Static Operation Delegates**: The method uses static operation delegates defined in `PradOp` (like `AddOp`, `MulOp`, `SinOp`, etc.) to determine which operation to apply. These static delegates act as keys to retrieve the corresponding instance method.

3. **Flexible Input**: The method can handle operations that require no additional input, a single additional tensor, or multiple additional tensors and an axis.

4. **Dynamic Dispatch**: The method uses the `GetOperation<T>` method of `PradOp` to dynamically retrieve the correct instance method based on the static delegate provided.

 ### PradResult.ThenParallel Method

 The `ThenParallel` method allows for parallel execution of multiple operations on the same PradResult. This is useful for creating branching computational graphs where multiple operations are performed on the same input.

 #### Method Signature

 ```csharp
 public PradResult[] ThenParallel(params Func<PradResult, PradResult>[] operations)
 ```

 #### Functionality

 1. **Parallel Execution**: The method executes multiple operations in parallel, each operating on a copy of the current PradResult.
 2. **Branching**: It creates multiple branches in the computation graph, allowing for different operations to be applied to the same input.
 3. **Result Aggregation**: Returns an array of PradResult instances, one for each parallel operation.

### Usage Examples 

Here are some examples of how to use `PradOp`: 

```csharp 
// Create a PradOp instance with a seed tensor 
var seed = new Tensor(new double[,] { { 1, 2 }, { 3, 4 } }); 
var op = new PradOp(seed); 

// Perform operations 
var result = op.Add(new Tensor(new double[,] { { 5, 6 }, { 7, 8 } })) 
               .Then(PradOp.SquareRootOp) 
               .Then(PradOp.MulOp, new Tensor(new double[,] { { 2, 2 }, { 2, 2 } })); 

// Compute gradients 
op.Back(new Tensor(new double[,] { { 1, 1 }, { 1, 1 } })); 

// Access gradients
var seedGradient = op.SeedGradient;
```

#### Chaining Operations

PradResult allows for elegant chaining of operations:

```csharp
var x = new PradOp(inputTensor);
var y = someOtherTensor;
var result = x.SquareRoot().Then(PradOp.Add, y);
```

#### Parallel Operations

PradOp supports parallel execution of multiple operations:

```csharp -->
var (result1, result2) = pradOp.DoParallel( 
    x => x.Sin(), 
    x => x.Cos()
);
```

Here is a neural network layer with multiple activations:

This example demonstrates how to use ThenParallel and the Then overloads to compute a neural network layer with multiple activation functions in parallel.

```csharp
// Define input and weights
var input = new Tensor(new int[] { 1, 4 }, new double[] { 0.1, 0.2, 0.3, 0.4 });
var weights = new Tensor(new int[] { 4, 3 }, new double[] { 
    0.1, 0.2, 0.3,
    0.4, 0.5, 0.6,
    0.7, 0.8, 0.9,
    1.0, 1.1, 1.2
});
var bias = new Tensor(new int[] { 1, 3 }, new double[] { 0.1, 0.2, 0.3 });

// Create PradOp instance
var pradOp = new PradOp(input);

PradResult? weightsResult = null;
PradResult? biasResult = null;

// Compute layer output with multiple activations
var result = pradOp.MatMul(weights)
    .Then(result => {
        weightsResult = result;
        return result.Then(PradOp.AddOp, bias);
    })
    .Then(result => {
        biasResult = result;
        return result.ThenParallel(
            result => result.Then(PradOp.SinOp),       // Sine activation
            result => result.Then(PradOp.ReciprocalOp).Then(PradOp.AddOp, new Tensor(new int[] { 1, 3 }, 1)),
            result => result.Then(PradOp.ExpOp));        // Exponential activation
    })
    .Then(activations => {
        // Compute weighted sum of activations
        var weights = new Tensor(new int[] { 3 }, new double[] { 0.3, 0.3, 0.4 });
        return activations
            .Select((act, i) => act.PradOp.Mul(weights.Indexer($"{i}").BroadcastTo(new int[] { 1, 3 })))
            .Aggregate((a, b) => a.PradOp.Add(b.Result));
    });

// Compute gradient
var upstreamGradient = new Tensor(new int[] { 1, 3 }, new double[] { 1, 1, 1 });
var gradient = pradOp.Back(upstreamGradient);

// Access results and gradients
Console.WriteLine("Layer output: " + result.Result);
Console.WriteLine("Input gradient: " + gradient);
```

This example showcases:

1. **Matrix Multiplication and Bias Addition**: Simulating a basic neural network layer computation.
2. **Parallel Activation Functions**: Using ThenParallel to apply multiple activation functions to the layer output simultaneously.
3. **Result Aggregation**: Using the Then method to combine the results of multiple activations with a weighted sum.
4. **Gradient Computation**: Demonstrating how gradients can be computed through this complex computation graph.

This example illustrates how ThenParallel and the Then overloads can be used to create more complex and flexible computational graphs, such as those found in advanced neural network architectures with multiple parallel pathways.

#### Combining LessThan, Where, and Modulus

```csharp
// Create input tensors
var x = new Tensor(new int[] { 2, 3 }, new double[] { 1, 2, 3, 4, 5, 6 });
var y = new Tensor(new int[] { 2, 3 }, new double[] { 3, 3, 3, 3, 3, 3 });
var pradOp = new PradOp(x);

// Perform operations
var result = pradOp
    .LessThan(y)  // Check which elements of x are less than 3
    .Then(lessThanResult => {
        var lessThanResultBranch = lessThanResult.PradOp.Branch();
        var modulusResult = lessThanResult.PradOp.Modulus(new Tensor(new int[] { 2, 3 }, new double[] { 2, 2, 2, 2, 2, 2 }));
        return modulusResult.PradOp.Where(lessThanResultBranch.BranchInitialTensor, y);
    });

// Compute gradients
var upstreamGradient = new Tensor(new int[] { 2, 3 }, new double[] { 1, 1, 1, 1, 1, 1 });
var gradient = pradOp.Back(upstreamGradient);
```

#### Custom Operations 

PradOp allows you to define custom operations with their own forward and backward passes. Here's an example of a custom sigmoid operation: 

```csharp 
public PradResult CustomSigmoid() 
{ 
    return this.CustomOperation( 
        operation: input =>  
        { 
            var result = new Tensor(input.Shape); 
            for (int i = 0; i < input.Data.Length; i++) 
            { 
                result.Data[i] = 1 / (1 + PradMath.Exp(-input.Data[i])); 
            } 
            return result; 
        }, 
        reverseOperation: (input, output, upstreamGrad) =>  
        { 
            var gradient = new Tensor(input.Shape); 
            for (int i = 0; i < input.Data.Length; i++) 
            { 
                gradient.Data[i] = output.Data[i] * (1 - output.Data[i]) * upstreamGrad.Data[i]; 
            } 
            return new[] { gradient }; 
        }, 
        outputShape: this.currentTensor.Shape 
    ); 
} 
``` 

Usage: 

```csharp 
var pradOp = new PradOp(inputTensor); 
var result = pradOp.CustomSigmoid(); 
var gradient = pradOp.Back(upstreamGradient); 
``` 

#### Branching 

PradOp supports creating complex computational graphs with branching paths. Here's an example: 

```csharp 
var pradOp = new PradOp(inputTensor); 
var branch = pradOp.Branch(); 

var result1 = pradOp.Sin(); 
var result2 = branch.Cos(); 

var combinedResult = pradOp.Add(result2.Result); 
var gradient = pradOp.Back(upstreamGradient); 
``` 

#### BranchStack

The `BranchStack(int n)` method is designed to streamline the management of multiple branches within the computation graph. It creates `n` branches from the current `PradOp` instance and encapsulates them in a `BranchStack` object. This object provides a `Pop()` method to retrieve and work with individual branches in a controlled and orderly manner.

```csharp
var tBranches = t.BranchStack(4);

var t2 = t.Square();
var t3 = t2.Then(PradOp.MulOp, tBranches.Pop().BranchInitialTensor);
var t4 = t3.Then(PradOp.MulOp, tBranches.Pop().BranchInitialTensor);

var mt = tBranches.Pop().SubFrom(new Tensor(t.CurrentShape, 1.0));
var mt2 = mt.Square();
var mt3 = mt2.Then(PradOp.MulOp, tBranches.Pop().BranchInitialTensor);
```

#### Splitting 

PradOp allows you to split tensors and perform operations on the split parts. Here's an example: 

```csharp 
var pradOp = new PradOp(inputTensor); // Assume inputTensor has shape [4, 10] 
var (leftHalf, rightHalf) = pradOp.Split(5, axis: 1); // Split along the second dimension 

var processedLeft = leftHalf.Square(); 
var processedRight = rightHalf.SquareRoot(); 

var recombined = leftHalf.Stack(new[] { processedRight.Result }, axis: 1); 
var gradient = recombined.Back(upstreamGradient); 
``` 

These examples demonstrate the flexibility of PradOp in handling complex computational graphs, including custom operations, branching, and splitting/recombining tensors. The automatic differentiation system takes care of computing the correct gradients through these complex structures.
