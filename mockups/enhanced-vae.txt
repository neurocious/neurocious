using ParallelReverseAutoDiff.PRAD;

public class EnhancedVAE
{
    private const int INPUT_DIM = 784;
    private const int HIDDEN_DIM = 256;
    private const int LATENT_DIM = 32;
    private const int SEQUENCE_LENGTH = 16;  // For sequential inputs
    
    // Sequential encoder components
    private PradOp encoderInputProj;  // Project input to hidden dimension
    private List<PradOp> encoderSelfAttention;  // Self-attention layers
    private PradOp encoderLayerNorm;
    private PradOp encoderMean;
    private PradOp encoderLogVar;

    // Decoder components with field outputs
    private PradOp decoderFC1;
    private PradOp decoderFC2;
    private PradOp decoderOutput;        // Regular reconstruction
    private PradOp decoderFieldOutput;   // Field parameter output

    private double klWeight = 0.0;  // For KL annealing
    private readonly Random random = new Random();

    public EnhancedVAE()
    {
        InitializeNetworks();
    }

    private void InitializeNetworks()
    {
        // Initialize sequential encoder
        encoderInputProj = new PradOp(InitializeWeights(INPUT_DIM, HIDDEN_DIM));
        
        // Initialize self-attention layers
        encoderSelfAttention = new List<PradOp>();
        for (int i = 0; i < 3; i++)  // 3 attention layers
        {
            encoderSelfAttention.Add(new PradOp(InitializeWeights(HIDDEN_DIM, HIDDEN_DIM)));
        }
        
        encoderLayerNorm = new PradOp(InitializeWeights(HIDDEN_DIM, HIDDEN_DIM));
        encoderMean = new PradOp(InitializeWeights(HIDDEN_DIM, LATENT_DIM));
        encoderLogVar = new PradOp(InitializeWeights(HIDDEN_DIM, LATENT_DIM));

        // Initialize decoder
        decoderFC1 = new PradOp(InitializeWeights(LATENT_DIM, HIDDEN_DIM));
        decoderFC2 = new PradOp(InitializeWeights(HIDDEN_DIM, HIDDEN_DIM));
        decoderOutput = new PradOp(InitializeWeights(HIDDEN_DIM, INPUT_DIM));
        decoderFieldOutput = new PradOp(InitializeWeights(HIDDEN_DIM, 3));  // Field parameters (e.g., curvature, entropy, alignment)
    }

    private Tensor InitializeWeights(int inputDim, int outputDim)
    {
        var stddev = Math.Sqrt(2.0 / (inputDim + outputDim));
        var weights = new double[inputDim * outputDim];
        for (int i = 0; i < weights.Length; i++)
        {
            weights[i] = random.NextGaussian(0, stddev);
        }
        return new Tensor(new[] { inputDim, outputDim }, weights);
    }

    public (PradResult mean, PradResult logVar) EncodeSequence(List<PradOp> sequence)
    {
        // Project each input to hidden dimension
        var projectedSequence = sequence.Select(input => 
            encoderInputProj.MatMul(input).Then(PradOp.LeakyReLUOp)).ToList();

        // Apply self-attention layers
        foreach (var attentionLayer in encoderSelfAttention)
        {
            var attentionScores = new List<PradResult>();
            
            // Compute attention scores for each position
            for (int i = 0; i < projectedSequence.Count; i++)
            {
                var scores = projectedSequence.Select(j => 
                    attentionLayer.MatMul(projectedSequence[i].Result)
                        .Then(x => x.MatMul(j.Result.Transpose()))
                        .Then(PradOp.SoftmaxOp));
                attentionScores.AddRange(scores);
            }

            // Apply attention and residual connection
            for (int i = 0; i < projectedSequence.Count; i++)
            {
                var weightedSum = attentionScores[i].Result
                    .ElementwiseMultiply(projectedSequence[i].Result);
                projectedSequence[i] = new PradOp(weightedSum)
                    .Add(projectedSequence[i].Result);
            }
        }

        // Layer normalization and mean pooling
        var normalized = projectedSequence.Select(x => 
            encoderLayerNorm.MatMul(x.Result).Then(PradOp.LeakyReLUOp));
        
        var pooled = normalized.Aggregate((a, b) => 
            new PradOp(a.Result).Add(b.Result))
            .Then(x => x.Div(new Tensor(x.Result.Shape, sequence.Count)));

        // Project to latent parameters
        var mean = encoderMean.MatMul(pooled.Result);
        var logVar = encoderLogVar.MatMul(pooled.Result);

        return (mean, logVar);
    }

    public (PradResult reconstruction, PradResult fieldParams) DecodeWithField(PradOp latentVector)
    {
        var hidden1 = decoderFC1.MatMul(latentVector)
            .Then(PradOp.LeakyReLUOp);

        var hidden2 = decoderFC2.MatMul(hidden1.Result)
            .Then(PradOp.LeakyReLUOp);

        // Dual output: reconstruction and field parameters
        var reconstruction = decoderOutput.MatMul(hidden2.Result)
            .Then(PradOp.SigmoidOp);
        
        var fieldParams = decoderFieldOutput.MatMul(hidden2.Result)
            .Then(PradOp.TanhOp);  // Field parameters in [-1, 1]

        return (reconstruction, fieldParams);
    }

    public (PradResult reconstruction, PradResult fieldParams, PradResult mean, PradResult logVar) 
        ForwardSequence(List<PradOp> sequence)
    {
        // Encode sequence to latent distribution
        var (mean, logVar) = EncodeSequence(sequence);

        // Sample latent vector using reparameterization trick
        var latentVector = ReparameterizationTrick(mean, logVar);

        // Decode with field parameters
        var (reconstruction, fieldParams) = DecodeWithField(new PradOp(latentVector.Result));

        return (reconstruction, fieldParams, mean, logVar);
    }

    public double EstimateLatentCurvature(Tensor z)
    {
        // Estimate local curvature in latent space using second derivatives
        var gradients = new double[z.Data.Length];
        var epsilon = 1e-5;

        for (int i = 0; i < z.Data.Length; i++)
        {
            var zPlus = z.Data.ToArray();
            var zMinus = z.Data.ToArray();
            zPlus[i] += epsilon;
            zMinus[i] -= epsilon;

            var reconstructionPlus = DecodeWithField(new PradOp(new Tensor(z.Shape, zPlus))).reconstruction;
            var reconstructionMinus = DecodeWithField(new PradOp(new Tensor(z.Shape, zMinus))).reconstruction;

            gradients[i] = (reconstructionPlus.Result.Data[0] - 2 * z.Data[i] + 
                reconstructionMinus.Result.Data[0]) / (epsilon * epsilon);
        }

        // Return mean absolute curvature
        return gradients.Select(Math.Abs).Average();
    }

    public double EstimateLatentEntropy(Tensor z)
    {
        // Estimate entropy using local neighborhood sampling
        var samples = new List<double>();
        var numSamples = 100;
        var radius = 0.1;

        for (int i = 0; i < numSamples; i++)
        {
            var noise = new double[z.Data.Length];
            for (int j = 0; j < noise.Length; j++)
            {
                noise[j] = random.NextGaussian(0, radius);
            }
            
            var neighborZ = z.Add(new Tensor(z.Shape, noise));
            var (recon, _) = DecodeWithField(new PradOp(neighborZ));
            samples.Add(recon.Result.Data.Average());
        }

        // Compute entropy estimate using histogram method
        var histogram = new Dictionary<int, int>();
        var binSize = 0.1;
        foreach (var sample in samples)
        {
            var bin = (int)(sample / binSize);
            if (!histogram.ContainsKey(bin))
                histogram[bin] = 0;
            histogram[bin]++;
        }

        var entropy = 0.0;
        foreach (var count in histogram.Values)
        {
            var p = count / (double)samples.Count;
            entropy -= p * Math.Log(p);
        }

        return entropy;
    }

    private PradResult ComputeLossWithKLAnnealing(
        List<Tensor> inputs, 
        PradResult reconstruction, 
        PradResult fieldParams,
        PradResult mean, 
        PradResult logVar,
        int epoch)
    {
        // Compute reconstruction loss
        var reconLoss = ComputeBatchReconstructionLoss(inputs, reconstruction);

        // Compute KL loss with annealing
        var klLoss = ComputeKLDivergenceLoss(mean, logVar);
        
        // Anneal KL weight from 0 to 1 over epochs
        klWeight = Math.Min(1.0, epoch / 50.0);  // Reach full weight at epoch 50

        // Add field regularization loss (optional)
        var fieldRegLoss = ComputeFieldRegularizationLoss(fieldParams);

        // Combine losses
        return new PradOp(reconLoss.Result)
            .Add(klLoss.Result.ElementwiseMultiply(new Tensor(klLoss.Result.Shape, klWeight)))
            .Add(fieldRegLoss.Result);
    }

    private PradResult ComputeBatchReconstructionLoss(List<Tensor> inputs, PradResult reconstruction)
    {
        var batchLoss = new List<PradResult>();
        for (int i = 0; i < inputs.Count; i++)
        {
            batchLoss.Add(ComputeBCELoss(reconstruction.Result, inputs[i]));
        }

        // Average losses across batch
        return batchLoss.Aggregate((a, b) => 
            new PradOp(a.Result).Add(b.Result))
            .Then(x => x.Div(new Tensor(x.Result.Shape, inputs.Count)));
    }

    private PradResult ComputeFieldRegularizationLoss(PradResult fieldParams)
    {
        // Encourage smooth field parameter transitions
        return new PradOp(fieldParams.Result)
            .Then(x => x.Square())
            .Then(x => x.Mean(0))
            .Then(x => x.Mul(new Tensor(x.Result.Shape, 0.1)));  // Small weight for regularization
    }

    public void Train(List<List<Tensor>> sequenceData, int epochs, int batchSize = 32)
    {
        var optimizer = new AdamOptimizer(0.001, 0.9, 0.999);

        for (int epoch = 0; epoch < epochs; epoch++)
        {
            double totalLoss = 0;
            
            // Process mini-batches of sequences
            for (int i = 0; i < sequenceData.Count; i += batchSize)
            {
                var batch = sequenceData.Skip(i).Take(batchSize).ToList();
                
                foreach (var sequence in batch)
                {
                    // Convert sequence to PradOp
                    var sequenceOps = sequence.Select(x => new PradOp(x)).ToList();
                    
                    // Forward pass
                    var (reconstruction, fieldParams, mean, logVar) = ForwardSequence(sequenceOps);

                    // Compute loss with KL annealing
                    var loss = ComputeLossWithKLAnnealing(
                        sequence, reconstruction, fieldParams, mean, logVar, epoch);
                    totalLoss += loss.Result.Data[0];

                    // Backward pass
                    loss.Back();

                    // Update weights
                    optimizer.Optimize(encoderInputProj);
                    foreach (var layer in encoderSelfAttention)
                        optimizer.Optimize(layer);
                    optimizer.Optimize(encoderLayerNorm);
                    optimizer.Optimize(encoderMean);
                    optimizer.Optimize(encoderLogVar);
                    optimizer.Optimize(decoderFC1);
                    optimizer.Optimize(decoderFC2);
                    optimizer.Optimize(decoderOutput);
                    optimizer.Optimize(decoderFieldOutput);
                }
            }

            if (epoch % 10 == 0)
            {
                Console.WriteLine($"Epoch {epoch}, Average Loss: {totalLoss / sequenceData.Count}, KL Weight: {klWeight:F3}");
            }
        }
    }
}
