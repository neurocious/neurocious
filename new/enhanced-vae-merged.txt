using ParallelReverseAutoDiff.PRAD;

public class EnhancedVAE
{
    protected const int INPUT_DIM = 784;
    protected const int HIDDEN_DIM = 256;
    protected const int LATENT_DIM = 32;
    protected const int SEQUENCE_LENGTH = 16;  // For sequential inputs
    protected const int NUM_HEADS = 4;
    protected const double DROPOUT_RATE = 0.1;

    // Sequential encoder components 
    protected PradOp encoderInputProj;  // Project input to hidden dimension
    protected List<AttentionBlock> encoderAttentionBlocks;
    protected PradOp encoderLayerNorm;
    protected PradOp encoderMean;
    protected PradOp encoderLogVar;

    // Decoder components with field outputs
    protected PradOp decoderFC1;
    protected PradOp decoderFC2; 
    protected PradOp decoderOutput;        // Regular reconstruction
    protected PradOp decoderFieldOutput;   // Field parameter output
    protected FieldRegularizer fieldRegularizer;

    protected double klWeight = 0.0;  // For KL annealing
    protected readonly Random random = new Random();

    public EnhancedVAE()
    {
        InitializeNetworks();
    }

    protected virtual void InitializeNetworks()
    {
        // Initialize sequential encoder
        encoderInputProj = new PradOp(InitializeWeights(INPUT_DIM, HIDDEN_DIM));
        
        // Initialize multi-head attention blocks
        encoderAttentionBlocks = new List<AttentionBlock>();
        for (int i = 0; i < 3; i++)
        {
            encoderAttentionBlocks.Add(new AttentionBlock(HIDDEN_DIM));
        }
        
        encoderLayerNorm = new PradOp(InitializeWeights(HIDDEN_DIM, HIDDEN_DIM));
        encoderMean = new PradOp(InitializeWeights(HIDDEN_DIM, LATENT_DIM));
        encoderLogVar = new PradOp(InitializeWeights(HIDDEN_DIM, LATENT_DIM));

        // Initialize decoder
        decoderFC1 = new PradOp(InitializeWeights(LATENT_DIM, HIDDEN_DIM));
        decoderFC2 = new PradOp(InitializeWeights(HIDDEN_DIM, HIDDEN_DIM));
        decoderOutput = new PradOp(InitializeWeights(HIDDEN_DIM, INPUT_DIM));
        decoderFieldOutput = new PradOp(InitializeWeights(HIDDEN_DIM, 3));  // Field parameters (e.g., curvature, entropy, alignment)
        
        // Initialize field regularizer
        fieldRegularizer = new FieldRegularizer();
    }

    private Tensor InitializeWeights(int inputDim, int outputDim)
    {
        var stddev = Math.Sqrt(2.0 / (inputDim + outputDim));
        var weights = new double[inputDim * outputDim];
        for (int i = 0; i < weights.Length; i++)
        {
            weights[i] = random.NextGaussian(0, stddev);
        }
        return new Tensor(new[] { inputDim, outputDim }, weights);
    }

    public virtual (PradResult mean, PradResult logVar) EncodeSequence(List<PradOp> sequence)
    {
        // Project each input to hidden dimension
        var projectedSequence = sequence.Select(input => 
            encoderInputProj.MatMul(input).Then(PradOp.LeakyReLUOp)).ToList();

        // Apply multi-head attention blocks
        var processedSequence = projectedSequence;
        foreach (var attentionBlock in encoderAttentionBlocks)
        {
            processedSequence = processedSequence.Select(x => 
                attentionBlock.Forward(x, training: true)).ToList();
        }

        // Layer normalization and mean pooling
        var normalized = processedSequence.Select(x => 
            encoderLayerNorm.MatMul(x.Result).Then(PradOp.LeakyReLUOp));
        
        var pooled = normalized.Aggregate((a, b) => 
            new PradOp(a.Result).Add(b.Result))
            .Then(x => x.Div(new Tensor(x.Result.Shape, sequence.Count)));

        // Project to latent parameters
        var mean = encoderMean.MatMul(pooled.Result);
        var logVar = encoderLogVar.MatMul(pooled.Result);

        return (mean, logVar);
    }

    protected virtual (PradResult reconstruction, FieldParameters fieldParams) DecodeWithField(PradOp latentVector)
    {
        var hidden1 = decoderFC1.MatMul(latentVector)
            .Then(PradOp.LeakyReLUOp);

        var hidden2 = decoderFC2.MatMul(hidden1.Result)
            .Then(PradOp.LeakyReLUOp);

        // Regular reconstruction
        var reconstruction = decoderOutput.MatMul(hidden2.Result)
            .Then(PradOp.SigmoidOp);
        
        // Field parameters with specific activation functions
        var rawFieldParams = decoderFieldOutput.MatMul(hidden2.Result);
        
        // Apply specific activation functions for each field parameter
        var curvature = new PradOp(rawFieldParams.Result.Indexer("0"))
            .Then(PradOp.ReluOp);  // Non-negative curvature
            
        var entropy = new PradOp(rawFieldParams.Result.Indexer("1"))
            .Then(PradOp.SigmoidOp);  // Bounded [0,1]
            
        var alignment = new PradOp(rawFieldParams.Result.Indexer("2"))
            .Then(PradOp.TanhOp);  // Directional [-1,1]

        var fieldParams = new FieldParameters(new Tensor(
            new[] { 3 },
            new[] { 
                curvature.Result.Data[0],
                entropy.Result.Data[0],
                alignment.Result.Data[0]
            }));

        return (reconstruction, fieldParams);
    }

    public virtual (PradResult reconstruction, FieldParameters fieldParams, PradResult mean, PradResult logVar) 
        ForwardSequence(List<PradOp> sequence)
    {
        // Encode sequence to latent distribution
        var (mean, logVar) = EncodeSequence(sequence);

        // Sample latent vector using reparameterization trick
        var latentVector = ReparameterizationTrick(mean, logVar);

        // Decode with field parameters
        var (reconstruction, fieldParams) = DecodeWithField(new PradOp(latentVector.Result));

        return (reconstruction, fieldParams, mean, logVar);
    }

    public double EstimateLatentCurvature(Tensor z)
    {
        // Estimate local curvature in latent space using second derivatives
        var gradients = new double[z.Data.Length];
        var epsilon = 1e-5;

        for (int i = 0; i < z.Data.Length; i++)
        {
            var zPlus = z.Data.ToArray();
            var zMinus = z.Data.ToArray();
            zPlus[i] += epsilon;
            zMinus[i] -= epsilon;

            var reconstructionPlus = DecodeWithField(new PradOp(new Tensor(z.Shape, zPlus))).reconstruction;
            var reconstructionMinus = DecodeWithField(new PradOp(new Tensor(z.Shape, zMinus))).reconstruction;

            gradients[i] = (reconstructionPlus.Result.Data[0] - 2 * z.Data[i] + 
                reconstructionMinus.Result.Data[0]) / (epsilon * epsilon);
        }

        // Return mean absolute curvature
        return gradients.Select(Math.Abs).Average();
    }

    public double EstimateLatentEntropy(Tensor z)
    {
        // Estimate entropy using local neighborhood sampling
        var samples = new List<double>();
        var numSamples = 100;
        var radius = 0.1;

        for (int i = 0; i < numSamples; i++)
        {
            var noise = new double[z.Data.Length];
            for (int j = 0; j < noise.Length; j++)
            {
                noise[j] = random.NextGaussian(0, radius);
            }
            
            var neighborZ = z.Add(new Tensor(z.Shape, noise));
            var (recon, _) = DecodeWithField(new PradOp(neighborZ));
            samples.Add(recon.Result.Data.Average());
        }

        // Compute entropy estimate using histogram method
        var histogram = new Dictionary<int, int>();
        var binSize = 0.1;
        foreach (var sample in samples)
        {
            var bin = (int)(sample / binSize);
            if (!histogram.ContainsKey(bin))
                histogram[bin] = 0;
            histogram[bin]++;
        }

        var entropy = 0.0;
        foreach (var count in histogram.Values)
        {
            var p = count / (double)samples.Count;
            entropy -= p * Math.Log(p);
        }

        return entropy;
    }

    protected virtual PradResult ComputeLoss(
        List<Tensor> inputs, 
        PradResult reconstruction, 
        FieldParameters fieldParams,
        PradResult mean, 
        PradResult logVar,
        int epoch)
    {
        // Compute reconstruction loss
        var reconLoss = ComputeBatchReconstructionLoss(inputs, reconstruction);

        // Compute KL loss with annealing
        var klLoss = ComputeKLDivergenceLoss(mean, logVar);
        
        // Anneal KL weight from 0 to 1 over epochs
        klWeight = Math.Min(1.0, epoch / 50.0);  // Reach full weight at epoch 50

        // Add field regularization losses
        var fieldRegLoss = fieldRegularizer.ComputeLoss(fieldParams);
        
        // Add contrastive loss for regime transitions
        var contrastiveLoss = ComputeContrastiveLoss(mean, inputs);

        // Combine losses with weights
        return new PradOp(reconLoss.Result)
            .Add(klLoss.Result.ElementwiseMultiply(new Tensor(klLoss.Result.Shape, klWeight)))
            .Add(fieldRegLoss.Result.Mul(new Tensor(fieldRegLoss.Result.Shape, 0.1)))
            .Add(contrastiveLoss.Result.Mul(new Tensor(contrastiveLoss.Result.Shape, 0.05)));
    }

    private PradResult ComputeBatchReconstructionLoss(List<Tensor> inputs, PradResult reconstruction)
    {
        var batchLoss = new List<PradResult>();
        for (int i = 0; i < inputs.Count; i++)
        {
            batchLoss.Add(ComputeBCELoss(reconstruction.Result, inputs[i]));
        }

        // Average losses across batch
        return batchLoss.Aggregate((a, b) => 
            new PradOp(a.Result).Add(b.Result))
            .Then(x => x.Div(new Tensor(x.Result.Shape, inputs.Count)));
    }

    protected PradResult ComputeContrastiveLoss(PradResult latentMean, List<Tensor> inputs)
    {
        // Simple contrastive loss using cosine similarity
        var similarities = new List<PradResult>();
        
        for (int i = 0; i < inputs.Count; i++)
        {
            for (int j = i + 1; j < inputs.Count; j++)
            {
                var similarity = new PradOp(latentMean.Result)
                    .MatMul(new PradOp(inputs[j]).Transpose().Result)
                    .Then(x => x.Div(
                        new PradOp(latentMean.Result).Then(PradOp.SquareRootOp).Result
                            .ElementwiseMultiply(
                                new PradOp(inputs[j]).Then(PradOp.SquareRootOp).Result
                            )
                    ));
                    
                similarities.Add(similarity);
            }
        }

        // Average similarity
        return similarities.Aggregate((a, b) => 
            new PradOp(a.Result).Add(b.Result))
            .Then(x => x.Div(new Tensor(x.Result.Shape, similarities.Count)));
    }

    public virtual void Train(List<List<Tensor>> sequenceData, int epochs, int batchSize = 32)
    {
        var optimizer = new AdamOptimizer(0.001, 0.9, 0.999);

        for (int epoch = 0; epoch < epochs; epoch++)
        {
            double totalLoss = 0;
            
            // Process mini-batches of sequences
            for (int i = 0; i < sequenceData.Count; i += batchSize)
            {
                var batch = sequenceData.Skip(i).Take(batchSize).ToList();
                
                foreach (var sequence in batch)
                {
                    // Convert sequence to PradOp
                    var sequenceOps = sequence.Select(x => new PradOp(x)).ToList();
                    
                    // Forward pass
                    var (reconstruction, fieldParams, mean, logVar) = ForwardSequence(sequenceOps);

                    // Compute loss
                    var loss = ComputeLoss(sequence, reconstruction, fieldParams, mean, logVar, epoch);
                    totalLoss += loss.Result.Data[0];

                    // Backward pass
                    loss.Back();

                    // Update weights
                    optimizer.Optimize(encoderInputProj);
                    foreach (var block in encoderAttentionBlocks)
                    {
                        optimizer.Optimize(block.QueryProj);
                        optimizer.Optimize(block.KeyProj);
                        optimizer.Optimize(block.ValueProj);
                        optimizer.Optimize(block.OutputProj);
                        optimizer.Optimize(block.LayerNorm);
                    }
                    optimizer.Optimize(encoderLayerNorm);
                    optimizer.Optimize(encoderMean);
                    optimizer.Optimize(encoderLogVar);
                    optimizer.Optimize(decoderFC1);
                    optimizer.Optimize(decoderFC2);
                    optimizer.Optimize(decoderOutput);
                    optimizer.Optimize(decoderFieldOutput);
                }
            }

            if (epoch % 10 == 0)
            {
                Console.WriteLine($"Epoch {epoch}, Average Loss: {totalLoss / sequenceData.Count}, KL Weight: {klWeight:F3}");
            }
        }
    }
}

public class FieldParameters
{
    public double Curvature { get; set; }  // Non-negative, measures regime instability
    public double Entropy { get; set; }    // [0,1], measures narrative uncertainty
    public double Alignment { get; set; }  // [-1,1], measures directional coherence

    public FieldParameters(Tensor fieldParams)
    {
        Curvature = fieldParams.Data[0];  // Applied ReLU
        Entropy = fieldParams.Data[1];     // Applied Sigmoid
        Alignment = fieldParams.Data[2];   // Applied Tanh
    }

    public Tensor ToTensor()
    {
        return new Tensor(new[] { 3 }, new[] { Curvature, Entropy, Alignment });
    }
}

public class AttentionBlock
{
    public PradOp QueryProj { get; private set; }
    public PradOp KeyProj { get; private set; }
    public PradOp ValueProj { get; private set; }
    public PradOp OutputProj { get; private set; }
    public PradOp LayerNorm { get; private set; }

    private readonly int hiddenDim;
    private readonly int numHeads;
    private readonly double dropoutRate;

    public AttentionBlock(int hiddenDim, int numHeads = 4, double dropoutRate = 0.1)
    {
        this.hiddenDim = hiddenDim;
        this.numHeads = numHeads;
        this.dropoutRate = dropoutRate;
        
        InitializeWeights();
    }

    private void InitializeWeights()
    {
        var dimPerHead = hiddenDim / numHeads;
        
        QueryProj = new PradOp(InitializeWeights(hiddenDim, hiddenDim));
        KeyProj = new PradOp(InitializeWeights(hiddenDim, hiddenDim));
        ValueProj = new PradOp(InitializeWeights(hiddenDim, hiddenDim));
        OutputProj = new PradOp(InitializeWeights(hiddenDim, hiddenDim));
        LayerNorm = new PradOp(InitializeWeights(hiddenDim, hiddenDim));
    }

    private Tensor InitializeWeights(int inputDim, int outputDim)
    {
        var stddev = Math.Sqrt(2.0 / (inputDim + outputDim));
        var weights = new double[inputDim * outputDim];
        var random = new Random();
        for (int i = 0; i < weights.Length; i++)
        {
            weights[i] = random.NextGaussian(0, stddev);
        }
        return new Tensor(new[] { inputDim, outputDim }, weights);
    }

    public PradResult Forward(PradOp input, bool training = true)
    {
        // Project to Q, K, V
        var queries = QueryProj.MatMul(input);
        var keys = KeyProj.MatMul(input);
        var values = ValueProj.MatMul(input);

        // Split into heads
        var queryHeads = SplitHeads(queries.Result);
        var keyHeads = SplitHeads(keys.Result);
        var valueHeads = SplitHeads(values.Result);

        // Scaled dot-product attention for each head
        var attentionHeads = new List<PradResult>();
        for (int h = 0; h < numHeads; h++)
        {
            var scaledDotProduct = new PradOp(queryHeads[h])
                .MatMul(new PradOp(keyHeads[h]).Transpose().Result)
                .Then(x => x.Mul(new Tensor(x.Result.Shape, 1.0 / Math.Sqrt(hiddenDim / numHeads))));

            // Apply causal mask if training
            if (training)
            {
                scaledDotProduct = ApplyCausalMask(scaledDotProduct);
            }

            var attention = scaledDotProduct
                .Then(PradOp.SoftmaxOp)
                .Then(attn => attn.MatMul(new PradOp(valueHeads[h]).Result));

            attentionHeads.Add(attention);
        }

        // Concatenate heads and project
        var concatenated = ConcatenateHeads(attentionHeads);
        var output = OutputProj.MatMul(concatenated.Result);

        // Add residual connection and layer norm
        return new PradOp(output.Result)
            .Add(input.Result)
            .Then(x => LayerNorm.MatMul(x.Result));
    }

    private List<Tensor> SplitHeads(Tensor input)
    {
        var dimPerHead = hiddenDim / numHeads;
        var heads = new List<Tensor>();
        
        for (int h = 0; h < numHeads; h++)
        {
            var head = new double[input.Shape[0] * dimPerHead];
            for (int i = 0; i < input.Shape[0]; i++)
            {
                for (int j = 0; j < dimPerHead; j++)
                {
                    head[i * dimPerHead + j] = input.Data[i * hiddenDim + h * dimPerHead + j];
                }
            }
            heads.Add(new Tensor(new[] { input.Shape[0], dimPerHead }, head));
        }
        
        return heads;
    }

    private PradResult ConcatenateHeads(List<PradResult> heads)
    {
        var dimPerHead = hiddenDim / numHeads;
        var concatenated = new double[heads[0].Result.Shape[0] * hiddenDim];
        
        for (int h = 0; h < numHeads; h++)
        {
            for (int i = 0; i < heads[h].Result.Shape[0]; i++)
            {
                for (int j = 0; j < dimPerHead; j++)
                {
                    concatenated[i * hiddenDim + h * dimPerHead + j] = 
                        heads[h].Result.Data[i * dimPerHead + j];
                }
            }
        }
        
        return new PradOp(new Tensor(new[] { heads[0].Result.Shape[0], hiddenDim }, concatenated));
    }

    private PradResult ApplyCausalMask(PradResult attention)
    {
        var shape = attention.Result.Shape;
        var mask = new double[shape[0] * shape[1]];
        
        for (int i = 0; i < shape[0]; i++)
        {
            for (int j = 0; j < shape[1]; j++)
            {
                mask[i * shape[1] + j] = j <= i ? 1 : float.NegativeInfinity;
            }
        }
        
        return new PradOp(attention.Result)
            .Add(new Tensor(shape, mask));
    }
}

public class FieldRegularizer
{
    public PradResult ComputeLoss(FieldParameters fieldParams)
    {
        var losses = new List<double>();
        
        // Curvature smoothness
        losses.Add(Math.Pow(fieldParams.Curvature, 2) * 0.1);
        
        // Entropy bounds
        losses.Add(Math.Max(0, fieldParams.Entropy - 1) * 10);
        losses.Add(Math.Max(0, -fieldParams.Entropy) * 10);
        
        // Alignment regularization
        losses.Add(Math.Pow(fieldParams.Alignment, 2) * 0.05);
        
        return new PradOp(new Tensor(new[] { 1 }, 
            new[] { losses.Average() }));
    }
}