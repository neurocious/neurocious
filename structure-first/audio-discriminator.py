import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import librosa
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass, field
import json
import os
from datetime import datetime
from collections import defaultdict
import torchaudio
import torchaudio.transforms as T
from scipy.signal import find_peaks
from sklearn.metrics import mean_squared_error
from pesq import pesq
import soundfile as sf


@dataclass
class AudioStructuralSignature:
    """Enhanced structural signature for audio spectrograms."""
    entropy: torch.Tensor      # Harmonic structure regularity
    alignment: torch.Tensor    # Temporal/spectral flow consistency  
    curvature: torch.Tensor    # Pitch transition smoothness
    
    # Audio-specific metrics
    harmonic_coherence: torch.Tensor    # Harmonic series alignment
    temporal_stability: torch.Tensor    # Frame-to-frame consistency
    spectral_flow: torch.Tensor         # Frequency domain continuity
    
    def __post_init__(self):
        """Validate tensor shapes match."""
        shapes = [
            self.entropy.shape, self.alignment.shape, self.curvature.shape,
            self.harmonic_coherence.shape, self.temporal_stability.shape, 
            self.spectral_flow.shape
        ]
        if not all(shape == shapes[0] for shape in shapes):
            raise ValueError("All structural metrics must have matching shapes")
    
    @property
    def device(self) -> torch.device:
        return self.entropy.device
    
    def to(self, device: torch.device) -> 'AudioStructuralSignature':
        """Move all tensors to specified device."""
        return AudioStructuralSignature(
            entropy=self.entropy.to(device),
            alignment=self.alignment.to(device),
            curvature=self.curvature.to(device),
            harmonic_coherence=self.harmonic_coherence.to(device),
            temporal_stability=self.temporal_stability.to(device),
            spectral_flow=self.spectral_flow.to(device)
        )
    
    def audio_statistics(self) -> Dict[str, torch.Tensor]:
        """Compute audio-specific statistical summary."""
        stats = {}
        fields = {
            'entropy': self.entropy,
            'alignment': self.alignment, 
            'curvature': self.curvature,
            'harmonic_coherence': self.harmonic_coherence,
            'temporal_stability': self.temporal_stability,
            'spectral_flow': self.spectral_flow
        }
        
        for name, field in fields.items():
            stats.update({
                f'{name}_mean': field.mean(dim=(-2, -1)),
                f'{name}_std': field.std(dim=(-2, -1)),
                f'{name}_max': field.amax(dim=(-2, -1)),
                f'{name}_min': field.amin(dim=(-2, -1)),
                # Audio-specific percentiles
                f'{name}_p25': torch.quantile(field.flatten(-2, -1), 0.25, dim=-1),
                f'{name}_p75': torch.quantile(field.flatten(-2, -1), 0.75, dim=-1),
            })
        return stats


class AudioSpectrogramProcessor:
    """Processes audio to log-mel spectrograms with proper normalization."""
    
    def __init__(self, 
                 sample_rate: int = 22050,
                 n_fft: int = 1024,
                 hop_length: int = 256,
                 n_mels: int = 80,
                 f_min: float = 0.0,
                 f_max: Optional[float] = None):
        
        self.sample_rate = sample_rate
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels
        self.f_min = f_min
        self.f_max = f_max or sample_rate // 2
        
        # Create mel spectrogram transform
        self.mel_transform = T.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            n_mels=n_mels,
            f_min=f_min,
            f_max=self.f_max,
            power=2.0,
            normalized=True
        )
        
        # Amplitude to decibel conversion
        self.amplitude_to_db = T.AmplitudeToDB(stype='power', top_db=80)
    
    def audio_to_mel_spectrogram(self, waveform: torch.Tensor) -> torch.Tensor:
        """Convert waveform to log-mel spectrogram."""
        # Ensure proper shape [batch, channels, time]
        if waveform.dim() == 1:
            waveform = waveform.unsqueeze(0).unsqueeze(0)
        elif waveform.dim() == 2:
            waveform = waveform.unsqueeze(0)
        
        # Convert to mel spectrogram
        mel_spec = self.mel_transform(waveform)
        
        # Convert to log scale
        log_mel = self.amplitude_to_db(mel_spec)
        
        # Normalize to [-1, 1] range for neural networks
        log_mel = (log_mel + 80) / 80  # Assuming top_db=80
        log_mel = 2 * log_mel - 1
        
        return log_mel
    
    def load_audio_file(self, filepath: str) -> torch.Tensor:
        """Load audio file and convert to tensor."""
        waveform, sr = torchaudio.load(filepath)
        
        # Resample if necessary
        if sr != self.sample_rate:
            resampler = T.Resample(sr, self.sample_rate)
            waveform = resampler(waveform)
        
        # Convert to mono if stereo
        if waveform.size(0) > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        
        return waveform


class AudioStructuralAnalyzer(nn.Module):
    """Enhanced structural analyzer for audio spectrograms."""
    
    def __init__(self, 
                 window_size: int = 5, 
                 sigma: float = 1.0,
                 harmonic_analysis: bool = True):
        super().__init__()
        self.window_size = window_size
        self.sigma = sigma
        self.harmonic_analysis = harmonic_analysis
        self.eps = 1e-10
        
        # Create analysis kernels
        gaussian_kernel = self._create_gaussian_kernel()
        self.register_buffer('gaussian_kernel', gaussian_kernel)
        
        sobel_x, sobel_y = self._create_sobel_kernels()
        self.register_buffer('sobel_x', sobel_x)
        self.register_buffer('sobel_y', sobel_y)
        
        # Audio-specific kernels
        if harmonic_analysis:
            harmonic_kernel = self._create_harmonic_analysis_kernel()
            self.register_buffer('harmonic_kernel', harmonic_kernel)
    
    def _create_gaussian_kernel(self) -> torch.Tensor:
        """Create normalized Gaussian kernel."""
        kernel = torch.zeros(self.window_size, self.window_size, dtype=torch.float32)
        center = self.window_size // 2
        
        y, x = torch.meshgrid(
            torch.arange(self.window_size, dtype=torch.float32) - center,
            torch.arange(self.window_size, dtype=torch.float32) - center,
            indexing='ij'
        )
        
        kernel = torch.exp(-(x**2 + y**2) / (2 * self.sigma**2))
        kernel = kernel / kernel.sum()
        
        return kernel.view(1, 1, self.window_size, self.window_size)
    
    def _create_sobel_kernels(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """Create Sobel kernels for gradient computation."""
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32) / 8.0
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32) / 8.0
        
        return sobel_x.view(1, 1, 3, 3), sobel_y.view(1, 1, 3, 3)
    
    def _create_harmonic_analysis_kernel(self) -> torch.Tensor:
        """Create kernel for harmonic series detection."""
        # Vertical kernel for detecting harmonic series (frequency relationships)
        kernel_size = 7
        harmonic_kernel = torch.zeros(kernel_size, 1, dtype=torch.float32)
        
        # Simple harmonic detection pattern
        harmonic_kernel[0, 0] = -1
        harmonic_kernel[kernel_size//2, 0] = 2
        harmonic_kernel[-1, 0] = -1
        
        return harmonic_kernel.view(1, 1, kernel_size, 1)
    
    def analyze_audio_spectrogram(self, spectrogram: torch.Tensor) -> AudioStructuralSignature:
        """
        Extract complete structural signature from audio spectrogram.
        
        Args:
            spectrogram: [B, C, freq_bins, time_frames] log-mel spectrogram
        
        Returns:
            AudioStructuralSignature with all structural metrics
        """
        B, C, F, T = spectrogram.shape
        
        # Process each channel and aggregate
        all_entropy = []
        all_alignment = []
        all_curvature = []
        all_harmonic_coherence = []
        all_temporal_stability = []
        all_spectral_flow = []
        
        for c in range(C):
            channel_spec = spectrogram[:, c:c+1]  # [B, 1, F, T]
            
            # Convert to vector field representation
            # Use gradients as basis for vector field
            grad_freq = F.conv2d(channel_spec, self.sobel_y, padding=1)  # Frequency gradient
            grad_time = F.conv2d(channel_spec, self.sobel_x, padding=1)  # Time gradient
            
            # Convert gradients to polar coordinates
            magnitude = torch.sqrt(grad_freq**2 + grad_time**2 + self.eps)
            angle = torch.atan2(grad_freq, grad_time + self.eps)
            
            # Create vector field [B, 2, F, T]
            vector_field = torch.cat([magnitude, angle], dim=1)
            
            # Compute standard structural metrics
            entropy = self._compute_entropy(vector_field)
            alignment = self._compute_alignment(vector_field)
            curvature = self._compute_curvature(vector_field)
            
            # Compute audio-specific metrics
            harmonic_coherence = self._compute_harmonic_coherence(channel_spec)
            temporal_stability = self._compute_temporal_stability(channel_spec)
            spectral_flow = self._compute_spectral_flow(channel_spec)
            
            all_entropy.append(entropy)
            all_alignment.append(alignment)
            all_curvature.append(curvature)
            all_harmonic_coherence.append(harmonic_coherence)
            all_temporal_stability.append(temporal_stability)
            all_spectral_flow.append(spectral_flow)
        
        # Aggregate across channels
        entropy_field = torch.stack(all_entropy, dim=1).mean(dim=1, keepdim=True)
        alignment_field = torch.stack(all_alignment, dim=1).mean(dim=1, keepdim=True)
        curvature_field = torch.stack(all_curvature, dim=1).mean(dim=1, keepdim=True)
        harmonic_field = torch.stack(all_harmonic_coherence, dim=1).mean(dim=1, keepdim=True)
        temporal_field = torch.stack(all_temporal_stability, dim=1).mean(dim=1, keepdim=True)
        spectral_field = torch.stack(all_spectral_flow, dim=1).mean(dim=1, keepdim=True)
        
        return AudioStructuralSignature(
            entropy=entropy_field,
            alignment=alignment_field,
            curvature=curvature_field,
            harmonic_coherence=harmonic_field,
            temporal_stability=temporal_field,
            spectral_flow=spectral_field
        )
    
    def _compute_entropy(self, vector_field: torch.Tensor) -> torch.Tensor:
        """Compute structure tensor entropy for audio vector field."""
        magnitudes = vector_field[:, 0:1]
        angles = vector_field[:, 1:2]
        
        # Convert to Cartesian
        vx = magnitudes * torch.cos(angles)
        vy = magnitudes * torch.sin(angles)
        
        # Structure tensor components
        vxx = vx * vx
        vyy = vy * vy
        vxy = vx * vy
        
        # Smooth with Gaussian kernel
        pad = self.window_size // 2
        vxx_smooth = F.conv2d(vxx, self.gaussian_kernel, padding=pad)
        vyy_smooth = F.conv2d(vyy, self.gaussian_kernel, padding=pad)
        vxy_smooth = F.conv2d(vxy, self.gaussian_kernel, padding=pad)
        
        # Eigenvalues
        trace = vxx_smooth + vyy_smooth
        det = vxx_smooth * vyy_smooth - vxy_smooth * vxy_smooth
        
        discriminant = torch.sqrt(torch.clamp(trace * trace - 4 * det, min=0.0) + self.eps)
        lambda1 = torch.clamp((trace + discriminant) / 2, min=self.eps)
        lambda2 = torch.clamp((trace - discriminant) / 2, min=self.eps)
        
        # Normalize and compute entropy
        sum_eig = lambda1 + lambda2 + self.eps
        p1, p2 = lambda1 / sum_eig, lambda2 / sum_eig
        
        entropy = -(p1 * torch.log(p1 + self.eps) + p2 * torch.log(p2 + self.eps)) / np.log(2.0)
        return torch.clamp(entropy, 0.0, 1.0)
    
    def _compute_alignment(self, vector_field: torch.Tensor) -> torch.Tensor:
        """Compute local alignment field for audio vectors."""
        angles = vector_field[:, 1:2]
        
        ux = torch.cos(angles)
        uy = torch.sin(angles)
        
        pad = self.window_size // 2
        ux_avg = F.conv2d(ux, self.gaussian_kernel, padding=pad)
        uy_avg = F.conv2d(uy, self.gaussian_kernel, padding=pad)
        
        alignment = torch.sqrt(ux_avg**2 + uy_avg**2 + self.eps)
        return torch.clamp(alignment, 0.0, 1.0)
    
    def _compute_curvature(self, vector_field: torch.Tensor) -> torch.Tensor:
        """Compute curvature field for audio vector field."""
        angles = vector_field[:, 1:2]
        
        ux = torch.cos(angles)
        uy = torch.sin(angles)
        
        # Compute gradients
        dudx = F.conv2d(ux, self.sobel_x, padding=1)
        dudy = F.conv2d(ux, self.sobel_y, padding=1)
        dvdx = F.conv2d(uy, self.sobel_x, padding=1)
        dvdy = F.conv2d(uy, self.sobel_y, padding=1)
        
        # Frobenius norm
        curvature = torch.sqrt(dudx**2 + dudy**2 + dvdx**2 + dvdy**2 + self.eps)
        
        # Smooth
        pad = self.window_size // 2
        curvature = F.conv2d(curvature, self.gaussian_kernel, padding=pad)
        
        return curvature
    
    def _compute_harmonic_coherence(self, spectrogram: torch.Tensor) -> torch.Tensor:
        """Compute harmonic series coherence in frequency domain."""
        if not self.harmonic_analysis:
            return torch.zeros_like(spectrogram)
        
        # Apply harmonic detection kernel
        harmonic_response = F.conv2d(spectrogram, self.harmonic_kernel, padding=(3, 0))
        
        # Measure consistency of harmonic patterns
        coherence = torch.abs(harmonic_response)
        
        # Normalize by local energy
        local_energy = F.conv2d(spectrogram**2, self.gaussian_kernel, padding=self.window_size//2)
        coherence = coherence / (local_energy + self.eps)
        
        return torch.clamp(coherence, 0.0, 1.0)
    
    def _compute_temporal_stability(self, spectrogram: torch.Tensor) -> torch.Tensor:
        """Compute frame-to-frame temporal stability."""
        # Compute temporal differences
        temp_diff = torch.diff(spectrogram, dim=-1, prepend=spectrogram[..., :1])
        
        # Measure stability as inverse of variation
        stability = 1.0 / (1.0 + torch.abs(temp_diff))
        
        # Smooth temporally
        temporal_kernel = torch.ones(1, 1, 1, 3, device=spectrogram.device) / 3
        stability = F.conv2d(stability, temporal_kernel, padding=(0, 1))
        
        return stability
    
    def _compute_spectral_flow(self, spectrogram: torch.Tensor) -> torch.Tensor:
        """Compute spectral flow consistency in frequency domain."""
        # Compute frequency differences
        freq_diff = torch.diff(spectrogram, dim=-2, prepend=spectrogram[..., :1, :])
        
        # Measure flow smoothness
        flow = torch.abs(freq_diff)
        
        # Smooth across frequency
        freq_kernel = torch.ones(1, 1, 3, 1, device=spectrogram.device) / 3
        flow = F.conv2d(flow, freq_kernel, padding=(1, 0))
        
        # Convert to flow quality measure (smooth = high quality)
        flow_quality = 1.0 / (1.0 + flow)
        
        return flow_quality


class AudioSFVNNDiscriminator(nn.Module):
    """
    Structure-First Vector Neuron Network Discriminator for Audio GANs.
    
    Specialized for detecting artifacts in generated audio spectrograms by
    analyzing structural properties like harmonic coherence and temporal flow.
    """
    
    def __init__(self,
                 input_channels: int = 1,  # Mono log-mel spectrogram
                 vector_channels: List[int] = [32, 64, 128, 256],
                 window_size: int = 5,
                 sigma: float = 1.0,
                 classifier_hidden: int = 256,
                 dropout_rate: float = 0.1,
                 multiscale_analysis: bool = True):
        super().__init__()
        
        self.input_channels = input_channels
        self.vector_channels = vector_channels
        self.multiscale_analysis = multiscale_analysis
        
        # Build vector neuron backbone for spectrogram analysis
        self.vector_layers = nn.ModuleList()
        in_ch = input_channels
        
        for i, out_ch in enumerate(vector_channels):
            # Adaptive downsampling for different spectrogram regions
            if i == 0:
                stride = 1  # Preserve initial resolution
            elif i == 1:
                stride = (2, 1)  # Downsample frequency, preserve time
            else:
                stride = 2  # Standard downsampling
            
            from vector_network import VectorNeuronLayer  # Import from your existing implementation
            
            self.vector_layers.append(
                VectorNeuronLayer(
                    in_channels=in_ch,
                    out_channels=out_ch,
                    kernel_size=3,
                    stride=stride,
                    padding=1,
                    magnitude_activation='relu',
                    angle_activation='tanh'
                )
            )
            in_ch = out_ch * 2  # Account for magnitude + angle channels
        
        # Multi-scale structural analysis
        if multiscale_analysis:
            self.structural_analyzers = nn.ModuleList([
                AudioStructuralAnalyzer(window_size=3, sigma=0.5),   # Fine-grained
                AudioStructuralAnalyzer(window_size=5, sigma=1.0),   # Medium
                AudioStructuralAnalyzer(window_size=7, sigma=1.5),   # Coarse
            ])
        else:
            self.structural_analyzer = AudioStructuralAnalyzer(window_size, sigma)
        
        # Audio-aware classification head
        self.classifier = AudioDiscriminatorHead(
            signature_channels=6,  # entropy, alignment, curvature, harmonic, temporal, spectral
            num_scales=3 if multiscale_analysis else 1,
            hidden_dim=classifier_hidden,
            dropout_rate=dropout_rate
        )
    
    def forward(self, spectrogram: torch.Tensor, 
                return_signature: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, List[AudioStructuralSignature]]]:
        """
        Forward pass through audio discriminator.
        
        Args:
            spectrogram: [B, C, freq_bins, time_frames] log-mel spectrogram
            return_signature: Whether to return structural signatures
            
        Returns:
            Real/fake probability [B, 1] or (probability, signatures) if return_signature=True
        """
        # Pass through vector neuron layers to build vector field
        vector_field = spectrogram
        for layer in self.vector_layers:
            vector_field = layer(vector_field)
        
        # Extract structural signatures at multiple scales
        if self.multiscale_analysis:
            signatures = []
            for analyzer in self.structural_analyzers:
                signature = analyzer.analyze_audio_spectrogram(vector_field)
                signatures.append(signature)
        else:
            signature = self.structural_analyzer.analyze_audio_spectrogram(vector_field)
            signatures = [signature]
        
        # Classify based on structural properties
        real_fake_prob = self.classifier(signatures)
        
        if return_signature:
            return real_fake_prob, signatures
        return real_fake_prob
    
    def compute_structural_distance(self, spec1: torch.Tensor, spec2: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Compute structural distance between two spectrograms.
        
        Useful for measuring how "fake" a generated spectrogram is compared to real ones.
        """
        with torch.no_grad():
            _, sigs1 = self.forward(spec1, return_signature=True)
            _, sigs2 = self.forward(spec2, return_signature=True)
            
            distances = {}
            
            for scale, (sig1, sig2) in enumerate(zip(sigs1, sigs2)):
                scale_distances = {}
                
                # Standard metrics
                scale_distances['entropy'] = F.mse_loss(sig1.entropy, sig2.entropy)
                scale_distances['alignment'] = F.mse_loss(sig1.alignment, sig2.alignment)
                scale_distances['curvature'] = F.mse_loss(sig1.curvature, sig2.curvature)
                
                # Audio-specific metrics
                scale_distances['harmonic_coherence'] = F.mse_loss(sig1.harmonic_coherence, sig2.harmonic_coherence)
                scale_distances['temporal_stability'] = F.mse_loss(sig1.temporal_stability, sig2.temporal_stability)
                scale_distances['spectral_flow'] = F.mse_loss(sig1.spectral_flow, sig2.spectral_flow)
                
                distances[f'scale_{scale}'] = scale_distances
            
            return distances


class AudioDiscriminatorHead(nn.Module):
    """Classification head specialized for audio structural signatures."""
    
    def __init__(self,
                 signature_channels: int = 6,
                 num_scales: int = 3,
                 hidden_dim: int = 256,
                 dropout_rate: float = 0.1):
        super().__init__()
        
        self.signature_channels = signature_channels
        self.num_scales = num_scales
        
        # Feature extraction from multi-scale signatures
        self.feature_extractors = nn.ModuleList()
        
        for scale in range(num_scales):
            # Each scale gets its own feature extraction pathway
            extractor = nn.Sequential(
                nn.Conv2d(signature_channels, hidden_dim // 4, 3, padding=1),
                nn.BatchNorm2d(hidden_dim // 4),
                nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((4, 4)),  # Fixed spatial size for concatenation
                nn.Flatten(),
                nn.Linear((hidden_dim // 4) * 16, hidden_dim // num_scales),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate)
            )
            self.feature_extractors.append(extractor)
        
        # Final classification layers
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 4, 1),
            nn.Sigmoid()  # Output probability
        )
    
    def forward(self, signatures: List[AudioStructuralSignature]) -> torch.Tensor:
        """
        Classify based on multi-scale structural signatures.
        
        Args:
            signatures: List of AudioStructuralSignature for different scales
            
        Returns:
            Real/fake probability [B, 1]
        """
        scale_features = []
        
        for scale, signature in enumerate(signatures):
            # Concatenate all structural metrics for this scale
            structure_tensor = torch.cat([
                signature.entropy,
                signature.alignment,
                signature.curvature,
                signature.harmonic_coherence,
                signature.temporal_stability,
                signature.spectral_flow
            ], dim=1)  # [B, 6, F, T]
            
            # Extract features for this scale
            features = self.feature_extractors[scale](structure_tensor)
            scale_features.append(features)
        
        # Concatenate features from all scales
        combined_features = torch.cat(scale_features, dim=1)
        
        # Final classification
        real_fake_prob = self.classifier(combined_features)
        
        return real_fake_prob


class AudioStructuralLoss(nn.Module):
    """
    Loss function that combines adversarial training with structural consistency.
    
    Encourages generated audio to have realistic structural properties.
    """
    
    def __init__(self,
                 adversarial_weight: float = 1.0,
                 structural_weight: float = 0.5,
                 perceptual_weight: float = 0.3,
                 feature_matching_weight: float = 0.2):
        super().__init__()
        
        self.adversarial_weight = adversarial_weight
        self.structural_weight = structural_weight
        self.perceptual_weight = perceptual_weight
        self.feature_matching_weight = feature_matching_weight
        
        # Loss components
        self.adversarial_loss = nn.BCELoss()
        self.structural_loss = nn.MSELoss()
        self.perceptual_loss = nn.L1Loss()
    
    def generator_loss(self,
                      fake_specs: torch.Tensor,
                      real_specs: torch.Tensor,
                      discriminator: AudioSFVNNDiscriminator) -> Dict[str, torch.Tensor]:
        """Compute generator loss with structural guidance."""
        
        # Adversarial loss
        fake_probs, fake_sigs = discriminator(fake_specs, return_signature=True)
        real_probs, real_sigs = discriminator(real_specs, return_signature=True)
        
        real_labels = torch.ones_like(fake_probs)
        adv_loss = self.adversarial_loss(fake_probs, real_labels)
        
        # Structural consistency loss
        struct_loss = 0.0
        for fake_sig, real_sig in zip(fake_sigs, real_sigs):
            # Encourage similar structural statistics
            struct_loss += self.structural_loss(
                fake_sig.entropy.mean(), real_sig.entropy.mean()
            )
            struct_loss += self.structural_loss(
                fake_sig.harmonic_coherence.mean(), real_sig.harmonic_coherence.mean()
            )
            struct_loss += self.structural_loss(
                fake_sig.temporal_stability.mean(), real_sig.temporal_stability.mean()
            )
        
        struct_loss /= len(fake_sigs)
        
        # Perceptual loss (direct spectrogram similarity)
        perceptual_loss = self.perceptual_loss(fake_specs, real_specs)
        
        # Feature matching loss (encourage similar intermediate representations)
        fm_loss = 0.0
        for fake_sig, real_sig in zip(fake_sigs, real_sigs):
            fm_loss += self.structural_loss(fake_sig.alignment, real_sig.alignment)
            fm_loss += self.structural_loss(fake_sig.curvature, real_sig.curvature)
        fm_loss /= len(fake_sigs)
        
        # Combine losses
        total_loss = (
            self.adversarial_weight * adv_loss +
            self.structural_weight * struct_loss +
            self.perceptual_weight * perceptual_loss +
            self.feature_matching_weight * fm_loss
        )
        
        return {
            'total': total_loss,
            'adversarial': adv_loss,
            'structural': struct_loss,
            'perceptual': perceptual_loss,
            'feature_matching': fm_loss
        }
    
    def discriminator_loss(self,
                          real_specs: torch.Tensor,
                          fake_specs: torch.Tensor,
                          discriminator: AudioSFVNNDiscriminator) -> Dict[str, torch.Tensor]:
        """Compute discriminator loss."""
        
        # Real samples
        real_probs = discriminator(real_specs)
        real_labels = torch.ones_like(real_probs)
        real_loss = self.adversarial_loss(real_probs, real_labels)
        
        # Fake samples
        fake_probs = discriminator(fake_specs.detach())  # Detach to avoid generator gradients
        fake_labels = torch.zeros_like(fake_probs)
        fake_loss = self.adversarial_loss(fake_probs, fake_labels)
        
        # Total discriminator loss
        total_loss = (real_loss + fake_loss) / 2
        
        return {
            'total': total_loss,
            'real_loss': real_loss,
            'fake_loss': fake_loss,
            'real_accuracy': (real_probs > 0.5).float().mean(),
            'fake_accuracy': (fake_probs < 0.5).float().mean()
        }


class AudioQualityMetrics:
    """Comprehensive audio quality metrics for evaluating generated spectrograms."""
    
    def __init__(self, sample_rate: int = 22050):
        self.sample_rate = sample_rate
        self.processor = AudioSpectrogramProcessor(sample_rate=sample_rate)
    
    def compute_structural_metrics(self,
                                 real_specs: torch.Tensor,
                                 fake_specs: torch.Tensor,
                                 discriminator: AudioSFVNNDiscriminator) -> Dict[str, float]:
        """Compute structural quality metrics."""
        
        with torch.no_grad():
            # Get structural signatures
            _, real_sigs = discriminator(real_specs, return_signature=True)
            _, fake_sigs = discriminator(fake_specs, return_signature=True)
            
            metrics = {}
            
            # Average across scales and batch
            for scale, (real_sig, fake_sig) in enumerate(zip(real_sigs, fake_sigs)):
                scale_metrics = {}
                
                # Entropy divergence (should be similar for realistic audio)
                entropy_div = F.kl_div(
                    F.log_softmax(fake_sig.entropy.flatten(), dim=0),
                    F.softmax(real_sig.entropy.flatten(), dim=0),
                    reduction='batchmean'
                ).item()
                scale_metrics['entropy_divergence'] = entropy_div
                
                # Harmonic coherence difference
                harmonic_diff = F.mse_loss(
                    fake_sig.harmonic_coherence.mean(),
                    real_sig.harmonic_coherence.mean()
                ).item()
                scale_metrics['harmonic_difference'] = harmonic_diff
                
                # Temporal stability comparison
                temporal_diff = F.mse_loss(
                    fake_sig.temporal_stability.mean(),
                    real_sig.temporal_stability.mean()
                ).item()
                scale_metrics['temporal_difference'] = temporal_diff
                
                # Overall structural distance
                struct_dist = (
                    F.mse_loss(fake_sig.entropy, real_sig.entropy) +
                    F.mse_loss(fake_sig.alignment, real_sig.alignment) +
                    F.mse_loss(fake_sig.curvature, real_sig.curvature) +
                    F.mse_loss(fake_sig.harmonic_coherence, real_sig.harmonic_coherence) +
                    F.mse_loss(fake_sig.temporal_stability, real_sig.temporal_stability) +
                    F.mse_loss(fake_sig.spectral_flow, real_sig.spectral_flow)
                ).item() / 6
                scale_metrics['structural_distance'] = struct_dist
                
                metrics[f'scale_{scale}'] = scale_metrics
            
            # Overall metrics
            metrics['overall_structural_distance'] = np.mean([
                metrics[f'scale_{s}']['structural_distance'] for s in range(len(real_sigs))
            ])
            
            return metrics
    
    def compute_frechet_audio_distance(self,
                                     real_specs: torch.Tensor,
                                     fake_specs: torch.Tensor) -> float:
        """
        Compute Fr√©chet Audio Distance using structural features.
        
        Similar to FID but for audio structural representations.
        """
        from scipy.linalg import sqrtm
        
        # Flatten spectrograms to feature vectors
        real_features = real_specs.view(real_specs.size(0), -1).cpu().numpy()
        fake_features = fake_specs.view(fake_specs.size(0), -1).cpu().numpy()
        
        # Compute statistics
        mu_real, sigma_real = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)
        mu_fake, sigma_fake = np.mean(fake_features, axis=0), np.cov(fake_features, rowvar=False)
        
        # Fr√©chet distance
        diff = mu_real - mu_fake
        covmean = sqrtm(sigma_real.dot(sigma_fake))
        
        if np.iscomplexobj(covmean):
            covmean = covmean.real
        
        fad = diff.dot(diff) + np.trace(sigma_real + sigma_fake - 2 * covmean)
        return float(fad)
    
    def compute_spectral_metrics(self,
                               real_specs: torch.Tensor,
                               fake_specs: torch.Tensor) -> Dict[str, float]:
        """Compute standard spectral domain metrics."""
        
        real_np = real_specs.cpu().numpy()
        fake_np = fake_specs.cpu().numpy()
        
        metrics = {}
        
        # Spectral centroid difference
        real_centroid = np.mean(real_np * np.arange(real_np.shape[-2])[:, None], axis=-2)
        fake_centroid = np.mean(fake_np * np.arange(fake_np.shape[-2])[:, None], axis=-2)
        metrics['centroid_difference'] = float(np.mean(np.abs(real_centroid - fake_centroid)))
        
        # Spectral bandwidth difference
        real_bandwidth = np.std(real_np, axis=-2)
        fake_bandwidth = np.std(fake_np, axis=-2)
        metrics['bandwidth_difference'] = float(np.mean(np.abs(real_bandwidth - fake_bandwidth)))
        
        # Log-mel reconstruction error
        metrics['log_mel_mse'] = float(np.mean((real_np - fake_np) ** 2))
        metrics['log_mel_mae'] = float(np.mean(np.abs(real_np - fake_np)))
        
        return metrics


class AudioSFVNNTrainer:
    """Complete training framework for Audio SF-VNN GAN systems."""
    
    def __init__(self,
                 generator: nn.Module,
                 discriminator: AudioSFVNNDiscriminator,
                 device: torch.device,
                 lr_g: float = 2e-4,
                 lr_d: float = 2e-4,
                 beta1: float = 0.5,
                 beta2: float = 0.999):
        
        self.generator = generator.to(device)
        self.discriminator = discriminator.to(device)
        self.device = device
        
        # Optimizers
        self.optim_g = torch.optim.Adam(
            self.generator.parameters(), lr=lr_g, betas=(beta1, beta2)
        )
        self.optim_d = torch.optim.Adam(
            self.discriminator.parameters(), lr=lr_d, betas=(beta1, beta2)
        )
        
        # Loss function
        self.loss_fn = AudioStructuralLoss()
        
        # Metrics
        self.metrics = AudioQualityMetrics()
        
        # Training history
        self.history = {
            'epoch': [], 'g_loss': [], 'd_loss': [],
            'structural_distance': [], 'fad_score': [],
            'real_accuracy': [], 'fake_accuracy': []
        }
    
    def train_epoch(self,
                   dataloader: torch.utils.data.DataLoader,
                   epoch: int) -> Dict[str, float]:
        """Train for one epoch."""
        
        self.generator.train()
        self.discriminator.train()
        
        epoch_metrics = defaultdict(list)
        
        for batch_idx, (real_audio, _) in enumerate(dataloader):
            real_audio = real_audio.to(self.device)
            batch_size = real_audio.size(0)
            
            # Convert to spectrograms (assuming generator outputs audio)
            # In practice, you'd use your specific generator's output format
            with torch.no_grad():
                real_specs = self.audio_to_spectrogram(real_audio)
            
            # Generate fake spectrograms
            # This is placeholder - replace with your generator's interface
            noise = torch.randn(batch_size, 128, device=self.device)  # Adjust noise dim
            fake_audio = self.generator(noise)
            fake_specs = self.audio_to_spectrogram(fake_audio)
            
            # Train Discriminator
            self.optim_d.zero_grad()
            d_losses = self.loss_fn.discriminator_loss(real_specs, fake_specs, self.discriminator)
            d_losses['total'].backward()
            self.optim_d.step()
            
            # Train Generator
            self.optim_g.zero_grad()
            g_losses = self.loss_fn.generator_loss(fake_specs, real_specs, self.discriminator)
            g_losses['total'].backward()
            self.optim_g.step()
            
            # Record metrics
            for key, value in {**g_losses, **d_losses}.items():
                if isinstance(value, torch.Tensor):
                    epoch_metrics[key].append(value.item())
                else:
                    epoch_metrics[key].append(value)
            
            if batch_idx % 50 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}: '
                      f'G_loss: {g_losses["total"].item():.4f}, '
                      f'D_loss: {d_losses["total"].item():.4f}, '
                      f'Struct: {g_losses["structural"].item():.4f}')
        
        # Average metrics
        avg_metrics = {key: np.mean(values) for key, values in epoch_metrics.items()}
        return avg_metrics
    
    def evaluate(self,
                dataloader: torch.utils.data.DataLoader,
                num_samples: int = 100) -> Dict[str, float]:
        """Evaluate model with comprehensive metrics."""
        
        self.generator.eval()
        self.discriminator.eval()
        
        real_specs_list = []
        fake_specs_list = []
        
        with torch.no_grad():
            for batch_idx, (real_audio, _) in enumerate(dataloader):
                if len(real_specs_list) * real_audio.size(0) >= num_samples:
                    break
                
                real_audio = real_audio.to(self.device)
                real_specs = self.audio_to_spectrogram(real_audio)
                
                # Generate fake samples
                noise = torch.randn(real_audio.size(0), 128, device=self.device)
                fake_audio = self.generator(noise)
                fake_specs = self.audio_to_spectrogram(fake_audio)
                
                real_specs_list.append(real_specs)
                fake_specs_list.append(fake_specs)
        
        # Concatenate all samples
        real_specs = torch.cat(real_specs_list, dim=0)[:num_samples]
        fake_specs = torch.cat(fake_specs_list, dim=0)[:num_samples]
        
        # Compute metrics
        structural_metrics = self.metrics.compute_structural_metrics(
            real_specs, fake_specs, self.discriminator
        )
        spectral_metrics = self.metrics.compute_spectral_metrics(real_specs, fake_specs)
        fad_score = self.metrics.compute_frechet_audio_distance(real_specs, fake_specs)
        
        return {
            **structural_metrics,
            **spectral_metrics,
            'fad_score': fad_score
        }
    
    def audio_to_spectrogram(self, audio: torch.Tensor) -> torch.Tensor:
        """Convert audio tensor to log-mel spectrogram."""
        processor = AudioSpectrogramProcessor()
        # Assuming audio is [B, 1, T] format
        if audio.dim() == 3:
            audio = audio.squeeze(1)  # Remove channel dim for processing
        
        specs = []
        for i in range(audio.size(0)):
            spec = processor.audio_to_mel_spectrogram(audio[i])
            specs.append(spec)
        
        return torch.cat(specs, dim=0)
    
    def train(self,
             train_loader: torch.utils.data.DataLoader,
             val_loader: torch.utils.data.DataLoader,
             num_epochs: int = 100,
             eval_every: int = 10,
             save_every: int = 25) -> Dict[str, List[float]]:
        """Complete training loop."""
        
        print("üéµ Starting Audio SF-VNN GAN Training")
        print("=" * 50)
        
        best_fad = float('inf')
        
        for epoch in range(num_epochs):
            # Training
            train_metrics = self.train_epoch(train_loader, epoch)
            
            # Evaluation
            if epoch % eval_every == 0:
                val_metrics = self.evaluate(val_loader)
                
                # Record history
                self.history['epoch'].append(epoch)
                self.history['g_loss'].append(train_metrics['total'])
                self.history['d_loss'].append(train_metrics.get('total', 0))  # Discriminator total
                self.history['structural_distance'].append(
                    val_metrics['overall_structural_distance']
                )
                self.history['fad_score'].append(val_metrics['fad_score'])
                
                print(f"\nEpoch {epoch} Evaluation:")
                print(f"  FAD Score: {val_metrics['fad_score']:.4f}")
                print(f"  Structural Distance: {val_metrics['overall_structural_distance']:.4f}")
                print(f"  Log-Mel MSE: {val_metrics['log_mel_mse']:.4f}")
                
                # Save best model
                if val_metrics['fad_score'] < best_fad:
                    best_fad = val_metrics['fad_score']
                    self.save_checkpoint(epoch, f'best_audio_sfvnn_epoch_{epoch}.pth')
                    print(f"  üéØ New best FAD score! Saved checkpoint.")
            
            # Periodic saves
            if epoch % save_every == 0 and epoch > 0:
                self.save_checkpoint(epoch, f'audio_sfvnn_epoch_{epoch}.pth')
        
        print(f"\nüéâ Training Complete! Best FAD: {best_fad:.4f}")
        return self.history
    
    def save_checkpoint(self, epoch: int, filename: str):
        """Save training checkpoint."""
        checkpoint = {
            'epoch': epoch,
            'generator_state_dict': self.generator.state_dict(),
            'discriminator_state_dict': self.discriminator.state_dict(),
            'optim_g_state_dict': self.optim_g.state_dict(),
            'optim_d_state_dict': self.optim_d.state_dict(),
            'history': self.history
        }
        torch.save(checkpoint, filename)


class AudioSFVNNExperiments:
    """Experimental framework for studying Audio SF-VNN effectiveness."""
    
    def __init__(self, results_dir: str = 'audio_sfvnn_experiments'):
        self.results_dir = results_dir
        os.makedirs(results_dir, exist_ok=True)
    
    def ablation_study(self,
                      train_loader: torch.utils.data.DataLoader,
                      val_loader: torch.utils.data.DataLoader,
                      generator_factory: callable,
                      device: torch.device) -> Dict[str, Dict]:
        """
        Comprehensive ablation study comparing different discriminator configurations.
        """
        
        print("üß™ Starting Audio SF-VNN Ablation Study")
        print("=" * 60)
        
        # Define experimental configurations
        configs = [
            {
                'name': 'Standard_CNN_Discriminator',
                'discriminator_type': 'cnn',
                'description': 'Baseline CNN discriminator'
            },
            {
                'name': 'SFVNN_Single_Scale',
                'discriminator_type': 'sfvnn',
                'multiscale_analysis': False,
                'description': 'SF-VNN with single-scale analysis'
            },
            {
                'name': 'SFVNN_Multi_Scale',
                'discriminator_type': 'sfvnn',
                'multiscale_analysis': True,
                'description': 'SF-VNN with multi-scale analysis'
            },
            {
                'name': 'SFVNN_Large_Window',
                'discriminator_type': 'sfvnn',
                'multiscale_analysis': True,
                'window_size': 7,
                'description': 'SF-VNN with larger analysis windows'
            },
            {
                'name': 'SFVNN_Structural_Heavy',
                'discriminator_type': 'sfvnn',
                'multiscale_analysis': True,
                'structural_weight': 1.0,
                'description': 'SF-VNN with increased structural loss weight'
            }
        ]
        
        results = {}
        
        for config in configs:
            print(f"\nüî¨ Running: {config['name']}")
            print(f"   {config['description']}")
            
            # Create discriminator
            if config['discriminator_type'] == 'cnn':
                discriminator = self._create_baseline_cnn_discriminator()
            else:
                discriminator = AudioSFVNNDiscriminator(
                    multiscale_analysis=config.get('multiscale_analysis', True),
                    window_size=config.get('window_size', 5)
                )
            
            # Create generator (same for all experiments)
            generator = generator_factory()
            
            # Train model
            trainer = AudioSFVNNTrainer(
                generator=generator,
                discriminator=discriminator,
                device=device
            )
            
            # Adjust loss weights if specified
            if 'structural_weight' in config:
                trainer.loss_fn.structural_weight = config['structural_weight']
            
            # Train for reduced epochs for ablation
            history = trainer.train(
                train_loader=train_loader,
                val_loader=val_loader,
                num_epochs=50,  # Reduced for ablation
                eval_every=5
            )
            
            # Final evaluation
            final_metrics = trainer.evaluate(val_loader, num_samples=200)
            
            results[config['name']] = {
                'config': config,
                'history': history,
                'final_metrics': final_metrics,
                'best_fad': min(history['fad_score']) if history['fad_score'] else float('inf')
            }
            
            print(f"   ‚úÖ Final FAD: {final_metrics['fad_score']:.4f}")
            print(f"   üìä Structural Distance: {final_metrics['overall_structural_distance']:.4f}")
        
        # Save results
        self._save_ablation_results(results)
        
        # Generate comparison report
        self._generate_ablation_report(results)
        
        return results
    
    def _create_baseline_cnn_discriminator(self) -> nn.Module:
        """Create baseline CNN discriminator for comparison."""
        class CNNDiscriminator(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv_layers = nn.Sequential(
                    nn.Conv2d(1, 32, 4, 2, 1), nn.LeakyReLU(0.2),
                    nn.Conv2d(32, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2),
                    nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),
                    nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),
                    nn.AdaptiveAvgPool2d(1),
                    nn.Flatten(),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                return self.conv_layers(x)
        
        return CNNDiscriminator()
    
    def _save_ablation_results(self, results: Dict):
        """Save ablation study results."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = os.path.join(self.results_dir, f'ablation_results_{timestamp}.json')
        
        # Convert tensors to serializable format
        serializable_results = {}
        for name, result in results.items():
            serializable_results[name] = {
                'config': result['config'],
                'final_metrics': {k: float(v) if isinstance(v, (torch.Tensor, np.ndarray)) else v 
                                for k, v in result['final_metrics'].items()},
                'best_fad': result['best_fad']
            }
        
        with open(filename, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        
        print(f"üìÅ Results saved to: {filename}")
    
    def _generate_ablation_report(self, results: Dict):
        """Generate comprehensive ablation study report."""
        
        print("\n" + "="*60)
        print("üèÜ AUDIO SF-VNN ABLATION STUDY RESULTS")
        print("="*60)
        
        # Sort by FAD score (lower is better)
        sorted_results = sorted(results.items(), key=lambda x: x[1]['best_fad'])
        
        print("\nüìä Performance Ranking (by FAD Score):")
        print("-" * 50)
        for rank, (name, result) in enumerate(sorted_results, 1):
            fad = result['best_fad']
            struct_dist = result['final_metrics']['overall_structural_distance']
            print(f"{rank}. {name}")
            print(f"   FAD Score: {fad:.4f}")
            print(f"   Structural Distance: {struct_dist:.4f}")
            print(f"   Description: {result['config']['description']}")
            print()
        
        # Key insights
        print("üîç Key Insights:")
        print("-" * 30)
        
        best_model = sorted_results[0]
        baseline_model = next((r for r in sorted_results if 'CNN' in r[0]), None)
        
        if baseline_model:
            improvement = baseline_model[1]['best_fad'] - best_model[1]['best_fad']
            improvement_pct = (improvement / baseline_model[1]['best_fad']) * 100
            
            print(f"‚Ä¢ Best model: {best_model[0]}")
            print(f"‚Ä¢ FAD improvement over baseline: {improvement:.4f} ({improvement_pct:.1f}%)")
        
        # Find best SF-VNN variant
        sfvnn_models = [r for r in sorted_results if 'SFVNN' in r[0]]
        if len(sfvnn_models) > 1:
            best_sfvnn = sfvnn_models[0]
            print(f"‚Ä¢ Best SF-VNN configuration: {best_sfvnn[0]}")
            print(f"‚Ä¢ Multi-scale analysis benefit: {'Multi_Scale' in best_sfvnn[0]}")
        
        print("\nüéØ Recommendations:")
        print("-" * 30)
        print("‚Ä¢ Use multi-scale structural analysis for best performance")
        print("‚Ä¢ Balance structural and adversarial loss weights carefully")
        print("‚Ä¢ SF-VNN shows consistent improvements in audio quality metrics")
        print("‚Ä¢ Structural consistency correlates with perceptual quality")


# Example usage and integration
def create_hifigan_with_sfvnn_discriminator():
    """
    Example: Replace HiFi-GAN discriminator with SF-VNN.
    
    This shows how to integrate SF-VNN into existing GAN architectures.
    """
    
    # Placeholder generator (replace with actual HiFi-GAN generator)
    class SimpleGenerator(nn.Module):
        def __init__(self):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(128, 256 * 4 * 4),
                nn.ReLU(),
                nn.Unflatten(1, (256, 4, 4)),
                nn.ConvTranspose2d(256, 128, 4, 2, 1),
                nn.BatchNorm2d(128), nn.ReLU(),
                nn.ConvTranspose2d(128, 64, 4, 2, 1),
                nn.BatchNorm2d(64), nn.ReLU(),
                nn.ConvTranspose2d(64, 1, 4, 2, 1),
                nn.Tanh()
            )
        
        def forward(self, z):
            return self.net(z)
    
    generator = SimpleGenerator()
    discriminator = AudioSFVNNDiscriminator(
        input_channels=1,
        vector_channels=[32, 64, 128, 256],
        multiscale_analysis=True
    )
    
    return generator, discriminator


if __name__ == "__main__":
    # Example experimental setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Test the system with dummy data
    processor = AudioSpectrogramProcessor()
    analyzer = AudioStructuralAnalyzer()
    
    # Create dummy spectrogram
    dummy_spec = torch.randn(2, 1, 80, 128)  # [batch, channels, freq, time]
    
    # Test structural analysis
    signature = analyzer.analyze_audio_spectrogram(dummy_spec)
    print(f"Entropy shape: {signature.entropy.shape}")
    print(f"Harmonic coherence shape: {signature.harmonic_coherence.shape}")
    
    # Test discriminator
    discriminator = AudioSFVNNDiscriminator()
    prob, sigs = discriminator(dummy_spec, return_signature=True)
    print(f"Discriminator output shape: {prob.shape}")
    print(f"Number of signature scales: {len(sigs)}")
    
    # Test metrics
    metrics = AudioQualityMetrics()
    dummy_real = torch.randn(4, 1, 80, 128)
    dummy_fake = torch.randn(4, 1, 80, 128)
    
    struct_metrics = metrics.compute_structural_metrics(dummy_real, dummy_fake, discriminator)
    print(f"Structural metrics: {struct_metrics}")
    
    fad = metrics.compute_frechet_audio_distance(dummy_real, dummy_fake)
    print(f"FAD score: {fad:.4f}")
    
    print("\nüéµ Audio SF-VNN System Ready!")
    print("Replace dummy generator with your actual audio generator and start training!")
