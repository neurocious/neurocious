#!/usr/bin/env python3
"""
Quick Evidence Test: Fast but meaningful comparison of SF-VNN vs Vanilla
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
from typing import Dict

# Import our components
from hifi import HiFiGANConfig, HiFiGANGenerator, AudioSFVNNDiscriminator
from empirical_comparison import VanillaCNNDiscriminator

def quick_stability_stress_test():
    """Quick test of stability under different learning rates."""
    
    print("âš¡ QUICK STABILITY STRESS TEST")
    print("Testing discriminator robustness to learning rate changes")
    print("=" * 55)
    
    device = "cpu"
    
    # Create simplified models for speed
    generator = HiFiGANGenerator(HiFiGANConfig()).to(device)
    
    sf_disc = AudioSFVNNDiscriminator(
        input_channels=1,
        vector_channels=[16, 32],  # Very small for speed
        window_size=3,
        multiscale_analysis=False  # Disable for speed
    ).to(device)
    
    vanilla_disc = VanillaCNNDiscriminator({
        'channels': [16, 32],
        'kernel_sizes': [(3, 5), (3, 4)],
        'strides': [(1, 1), (1, 2)]
    }).to(device)
    
    # Test learning rates (including aggressive ones)
    learning_rates = [1e-4, 5e-4, 1e-3, 3e-3]
    
    # Simple test audio
    real_audio = torch.sin(2 * np.pi * 440 * torch.linspace(0, 0.2, 4096)).unsqueeze(0).unsqueeze(0).repeat(2, 1, 1)
    
    results = {}
    
    for lr in learning_rates:
        print(f"\\nðŸ“Š LR: {lr}")
        
        # Reset model parameters
        for module in [sf_disc, vanilla_disc]:
            for layer in module.modules():
                if hasattr(layer, 'reset_parameters'):
                    layer.reset_parameters()
        
        sf_opt = torch.optim.Adam(sf_disc.parameters(), lr=lr)
        vanilla_opt = torch.optim.Adam(vanilla_disc.parameters(), lr=lr)
        criterion = nn.BCEWithLogitsLoss()
        
        sf_losses = []
        vanilla_losses = []
        sf_exploded = False
        vanilla_exploded = False
        
        # Quick 10-step training
        for step in range(10):
            
            # Generate fake audio (simplified)
            with torch.no_grad():
                noise_mel = torch.randn(2, 80, 8)  # Very small
                fake_audio = generator(noise_mel)
                if fake_audio.dim() == 2:
                    fake_audio = fake_audio.unsqueeze(1)
                
                # Use fixed size for speed
                fake_audio = fake_audio[..., :4096]
                real_batch = real_audio[..., :4096]
            
            # Simple mel conversion
            import torchaudio.transforms as T
            mel_transform = T.MelSpectrogram(n_mels=20, n_fft=256, hop_length=64)  # Very small
            try:
                real_mel = mel_transform(real_batch.squeeze(1)).unsqueeze(1)
                fake_mel = mel_transform(fake_audio.squeeze(1)).unsqueeze(1)
                
                min_w = min(real_mel.size(-1), fake_mel.size(-1))
                real_mel = real_mel[..., :min_w]
                fake_mel = fake_mel[..., :min_w]
            except:
                continue
            
            # Train SF-VNN
            try:
                sf_opt.zero_grad()
                sf_real_pred = sf_disc(real_mel)
                sf_fake_pred = sf_disc(fake_mel.detach())
                sf_loss = (criterion(sf_real_pred, torch.ones_like(sf_real_pred)) + 
                          criterion(sf_fake_pred, torch.zeros_like(sf_fake_pred))) / 2
                
                if torch.isnan(sf_loss) or torch.isinf(sf_loss) or sf_loss.item() > 100:
                    sf_exploded = True
                    break
                
                sf_loss.backward()
                sf_opt.step()
                sf_losses.append(sf_loss.item())
            except:
                sf_exploded = True
                break
            
            # Train Vanilla
            try:
                vanilla_opt.zero_grad()
                vanilla_real_pred = vanilla_disc(real_mel)
                vanilla_fake_pred = vanilla_disc(fake_mel.detach())
                vanilla_loss = (criterion(vanilla_real_pred, torch.ones_like(vanilla_real_pred)) + 
                               criterion(vanilla_fake_pred, torch.zeros_like(vanilla_fake_pred))) / 2
                
                if torch.isnan(vanilla_loss) or torch.isinf(vanilla_loss) or vanilla_loss.item() > 100:
                    vanilla_exploded = True
                    break
                
                vanilla_loss.backward()
                vanilla_opt.step()
                vanilla_losses.append(vanilla_loss.item())
            except:
                vanilla_exploded = True
                break
        
        # Analyze results
        sf_stable = not sf_exploded and len(sf_losses) >= 5
        vanilla_stable = not vanilla_exploded and len(vanilla_losses) >= 5
        
        sf_variance = np.var(sf_losses) if sf_stable else float('inf')
        vanilla_variance = np.var(vanilla_losses) if vanilla_stable else float('inf')
        
        winner = "SF-VNN" if sf_stable and (not vanilla_stable or sf_variance < vanilla_variance) else "Vanilla"
        
        results[f'lr_{lr}'] = {
            'sf_stable': sf_stable,
            'vanilla_stable': vanilla_stable,
            'sf_variance': sf_variance if sf_variance != float('inf') else None,
            'vanilla_variance': vanilla_variance if vanilla_variance != float('inf') else None,
            'winner': winner,
            'sf_final_loss': sf_losses[-1] if sf_losses else None,
            'vanilla_final_loss': vanilla_losses[-1] if vanilla_losses else None
        }
        
        print(f"   SF-VNN: {'âœ… Stable' if sf_stable else 'âŒ Exploded'}")
        print(f"   Vanilla: {'âœ… Stable' if vanilla_stable else 'âŒ Exploded'}")
        print(f"   Winner: {winner}")
    
    return results, learning_rates

def quick_pattern_adaptation_test():
    """Quick test of adaptation to different audio patterns."""
    
    print("\\nðŸŽµ QUICK PATTERN ADAPTATION TEST") 
    print("Testing how quickly discriminators adapt to new patterns")
    print("=" * 55)
    
    device = "cpu"
    
    # Create models
    generator = HiFiGANGenerator(HiFiGANConfig()).to(device)
    
    sf_disc = AudioSFVNNDiscriminator(
        input_channels=1,
        vector_channels=[16, 32],
        window_size=3,
        multiscale_analysis=False
    ).to(device)
    
    vanilla_disc = VanillaCNNDiscriminator({
        'channels': [16, 32],
        'kernel_sizes': [(3, 5), (3, 4)],
        'strides': [(1, 1), (1, 2)]
    }).to(device)
    
    # Create very different audio patterns
    t = torch.linspace(0, 0.2, 4096)
    patterns = {
        'sine': torch.sin(2 * np.pi * 440 * t),
        'noise': torch.randn(4096) * 0.5,
        'sweep': torch.sin(2 * np.pi * (200 + 1000 * t) * t)
    }
    
    results = {}
    
    for pattern_name, audio in patterns.items():
        print(f"\\nðŸŽ¼ Pattern: {pattern_name}")
        
        real_audio = audio.unsqueeze(0).unsqueeze(0).repeat(2, 1, 1)
        
        # Fresh optimizers
        sf_opt = torch.optim.Adam(sf_disc.parameters(), lr=2e-4)
        vanilla_opt = torch.optim.Adam(vanilla_disc.parameters(), lr=2e-4)
        criterion = nn.BCEWithLogitsLoss()
        
        # Quick 5-step adaptation test
        initial_discrimination = {'sf': None, 'vanilla': None}
        final_discrimination = {'sf': None, 'vanilla': None}
        
        for step in range(5):
            
            # Generate fake
            with torch.no_grad():
                noise_mel = torch.randn(2, 80, 8)
                fake_audio = generator(noise_mel)[..., :4096]
                if fake_audio.dim() == 2:
                    fake_audio = fake_audio.unsqueeze(1)
            
            # Convert to mel
            import torchaudio.transforms as T
            mel_transform = T.MelSpectrogram(n_mels=20, n_fft=256, hop_length=64)
            try:
                real_mel = mel_transform(real_audio.squeeze(1)).unsqueeze(1)
                fake_mel = mel_transform(fake_audio.squeeze(1)).unsqueeze(1)
                
                min_w = min(real_mel.size(-1), fake_mel.size(-1))
                real_mel = real_mel[..., :min_w]
                fake_mel = fake_mel[..., :min_w]
            except:
                continue
            
            # Measure discrimination before training
            with torch.no_grad():
                try:
                    sf_real_pred = torch.sigmoid(sf_disc(real_mel))
                    sf_fake_pred = torch.sigmoid(sf_disc(fake_mel))
                    sf_discrimination = abs(sf_real_pred.mean() - sf_fake_pred.mean()).item()
                    
                    vanilla_real_pred = torch.sigmoid(vanilla_disc(real_mel))
                    vanilla_fake_pred = torch.sigmoid(vanilla_disc(fake_mel))
                    vanilla_discrimination = abs(vanilla_real_pred.mean() - vanilla_fake_pred.mean()).item()
                    
                    if step == 0:
                        initial_discrimination['sf'] = sf_discrimination
                        initial_discrimination['vanilla'] = vanilla_discrimination
                    elif step == 4:
                        final_discrimination['sf'] = sf_discrimination
                        final_discrimination['vanilla'] = vanilla_discrimination
                except:
                    continue
            
            # Train one step
            try:
                # SF-VNN
                sf_opt.zero_grad()
                sf_loss = (criterion(sf_disc(real_mel), torch.ones_like(sf_disc(real_mel))) + 
                          criterion(sf_disc(fake_mel.detach()), torch.zeros_like(sf_disc(fake_mel.detach())))) / 2
                sf_loss.backward()
                sf_opt.step()
                
                # Vanilla
                vanilla_opt.zero_grad()
                vanilla_loss = (criterion(vanilla_disc(real_mel), torch.ones_like(vanilla_disc(real_mel))) + 
                               criterion(vanilla_disc(fake_mel.detach()), torch.zeros_like(vanilla_disc(fake_mel.detach())))) / 2
                vanilla_loss.backward()
                vanilla_opt.step()
            except:
                continue
        
        # Calculate improvement
        sf_improvement = (final_discrimination['sf'] or 0) - (initial_discrimination['sf'] or 0)
        vanilla_improvement = (final_discrimination['vanilla'] or 0) - (initial_discrimination['vanilla'] or 0)
        
        winner = "SF-VNN" if sf_improvement > vanilla_improvement else "Vanilla"
        
        results[pattern_name] = {
            'sf_initial': initial_discrimination['sf'],
            'sf_final': final_discrimination['sf'],
            'sf_improvement': sf_improvement,
            'vanilla_initial': initial_discrimination['vanilla'],
            'vanilla_final': final_discrimination['vanilla'],
            'vanilla_improvement': vanilla_improvement,
            'winner': winner
        }
        
        print(f"   SF-VNN improvement: {sf_improvement:.4f}")
        print(f"   Vanilla improvement: {vanilla_improvement:.4f}")
        print(f"   Adaptation winner: {winner}")
    
    return results

def create_compelling_summary(stability_results, adaptation_results, learning_rates):
    """Create final compelling summary."""
    
    print("\\nðŸ† COMPELLING EVIDENCE SUMMARY")
    print("=" * 50)
    
    evidence = []
    
    # Stability analysis
    sf_stability_wins = 0
    for lr in learning_rates:
        result = stability_results[f'lr_{lr}']
        if result['winner'] == 'SF-VNN':
            sf_stability_wins += 1
    
    if sf_stability_wins > len(learning_rates) / 2:
        evidence.append(f"Learning Rate Robustness: SF-VNN more stable in {sf_stability_wins}/{len(learning_rates)} LR tests")
        print(f"âœ… SF-VNN more robust to learning rate changes: {sf_stability_wins}/{len(learning_rates)} wins")
    
    # Check high learning rate specifically
    high_lr_stable = {}
    for lr in learning_rates:
        if lr >= 1e-3:  # High learning rate
            result = stability_results[f'lr_{lr}']
            high_lr_stable['sf'] = high_lr_stable.get('sf', 0) + (1 if result['sf_stable'] else 0)
            high_lr_stable['vanilla'] = high_lr_stable.get('vanilla', 0) + (1 if result['vanilla_stable'] else 0)
    
    if high_lr_stable.get('sf', 0) > high_lr_stable.get('vanilla', 0):
        evidence.append("High Learning Rate Tolerance: SF-VNN handles aggressive learning rates better")
        print("âœ… SF-VNN better at handling high learning rates")
    
    # Adaptation analysis
    sf_adaptation_wins = sum(1 for result in adaptation_results.values() if result['winner'] == 'SF-VNN')
    
    if sf_adaptation_wins > len(adaptation_results) / 2:
        evidence.append(f"Pattern Adaptation: SF-VNN adapts faster to {sf_adaptation_wins}/{len(adaptation_results)} audio patterns")
        print(f"âœ… SF-VNN adapts faster to new patterns: {sf_adaptation_wins}/{len(adaptation_results)} wins")
    
    # Final assessment
    print(f"\\nðŸŽ¯ COMPELLING EVIDENCE:")
    if evidence:
        for i, item in enumerate(evidence, 1):
            print(f"   {i}. {item}")
        
        print(f"\\nðŸŽ‰ FOUND {len(evidence)} KEY ADVANTAGES!")
        print("   Your Structure-First approach shows clear benefits!")
        
        if len(evidence) >= 2:
            print("   ðŸš€ STRONG EVIDENCE for methods paper!")
    else:
        print("   ðŸ¤” No clear advantages found in this quick test")
        print("   ðŸ’¡ Consider longer experiments or different metrics")
    
    return evidence

def main():
    """Run quick compelling evidence tests."""
    
    print("âš¡ QUICK COMPELLING EVIDENCE TEST")
    print("Fast but meaningful comparison of SF-VNN vs Vanilla")
    print("=" * 60)
    
    # Run tests
    stability_results, learning_rates = quick_stability_stress_test()
    adaptation_results = quick_pattern_adaptation_test()
    
    # Create summary
    evidence = create_compelling_summary(stability_results, adaptation_results, learning_rates)
    
    # Save results
    final_results = {
        'stability_results': stability_results,
        'adaptation_results': adaptation_results,
        'compelling_evidence': evidence,
        'summary': {
            'total_evidence_points': len(evidence),
            'strong_evidence': len(evidence) >= 2
        }
    }
    
    with open('quick_evidence_results.json', 'w') as f:
        json.dump(final_results, f, indent=2)
    
    print(f"\\nðŸ’¾ Results saved to: quick_evidence_results.json")
    print("âœ… Quick evidence test completed!")

if __name__ == "__main__":
    main()